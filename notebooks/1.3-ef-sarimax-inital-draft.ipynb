{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA:\n",
    "Sales (Training Data),\n",
    "Transactions, &\n",
    "Events\n",
    "\n",
    "OUTPUT: EDA & Sales Forcasting using seasonaility\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules and setup dependacinesd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eflanag\\AppData\\Local\\Temp\\ipykernel_13824\\3927126861.py:27: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n"
     ]
    }
   ],
   "source": [
    "# BASE\n",
    "# ------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "#!pip install xgboost\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model  import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit \n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "\n",
    "from pandas import datetime\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# DATA VISUALIZATION\n",
    "# ------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# CONFIGURATIONS\n",
    "# ------------------------------------------------------\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "folder = 'C:/Users/eflanag/OneDrive - FTI Consulting/Documents/Projects/Ibex_EF/IbexV2/ibexgit/kaggle-store-sales/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: c:\\Users\\eflanag\\OneDrive - FTI Consulting\\Documents\\Projects\\Ibex_EF\\IbexV2\\ibexgit\\kaggle-store-sales\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/eflanag/OneDrive - FTI Consulting/Documents/Projects/Ibex_EF/IbexV2/ibexgit/kaggle-store-sales')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Gloabl Path Variables\n",
    "# ------------------------------------------------------\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.abspath(os.path.join(current_directory, os.pardir))\n",
    "\n",
    "print(\"Parent directory:\", parent_directory)\n",
    "\n",
    "DATA_PATH = Path(parent_directory)\n",
    "RAW_PATH = DATA_PATH / 'raw'\n",
    "TRAIN_PATH = RAW_PATH / 'train.csv'\n",
    "TEST_PATH = RAW_PATH / 'test.csv'\n",
    "DATA_PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory: c:\\Users\\eflanag\\OneDrive - FTI Consulting\\Documents\\Projects\\Ibex_EF\\IbexV2\\ibexgit\\kaggle-store-sales\n",
      "(3029400, 81)\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "\n",
    "# Specify the absolute path to the data_setup.py file\n",
    "data_setup_path = '../src/data_setup.py'\n",
    "\n",
    "# Load the module from the file\n",
    "spec = importlib.util.spec_from_file_location('data_setup', data_setup_path)\n",
    "data_setup = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(data_setup)\n",
    "\n",
    "d = data_setup.get_data()\n",
    "metrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling v0.1 - Sales forcasting using seasonailtiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3029400, 81)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resample to reduce data for dev\n",
    "dFull = d\n",
    "#d = dFull.sample(frac=0.1, replace=True, random_state=1)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame(columns=['Model', 'family_encoded', 'cols', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2'])\n",
    "\n",
    "# preform test train split\n",
    "cols = ['test/train' ,'sales', 'store_nbr', 'type', 'family', 'date', 'month','day_of_month', 'week_of_year','day_of_week', 'dcoilwtico_interpolated']\n",
    "#cols = ['test/train' ,'sales', 'family', 'date', 'month','day_of_month', 'week_of_year','day_of_week', 'type', 'dcoilwtico_interpolated']\n",
    "\n",
    "dataTrainTest = d[cols]\n",
    "\n",
    "dataTrain = dataTrainTest[dataTrainTest['test/train'] ==  'train'].reset_index(drop=True)\n",
    "dataTest = dataTrainTest[dataTrainTest['test/train'] ==  'test'].reset_index(drop=True)\n",
    "\n",
    "dataTrain\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "dataTrain[\"family_encoded\"] = encoder.fit_transform(dataTrain[\"family\"])\n",
    "dataTrain.drop('family', axis=1, inplace=True)\n",
    "\n",
    "dataTrain[\"type\"]\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "dataTrain[\"type_encoded\"] = encoder.fit_transform(dataTrain[\"type\"])\n",
    "dataTrain.drop('type', axis=1, inplace=True)\n",
    "\n",
    "train_ = (dataTrain)\n",
    "test_ = (dataTest)\n",
    "\n",
    "#train_, test_ = train_test_split(dataTrain.dropna().reset_index(drop=True), test_size=0.2, random_state=42)\n",
    "\n",
    "cols_out = list(dataTrain.columns)\n",
    "cols_out.append('y_pred')\n",
    "\n",
    "\n",
    "del d\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "global dataTrain\n",
    "\n",
    "# Define a function to fit SARIMAX model\n",
    "def fit_sarimax(data, exogenous, so):\n",
    "    model = SARIMAX(data, order=(1, 0, 4), exog=exogenous, seasonal_order=so)\n",
    "    model_fit = model.fit(disp=False)\n",
    "    return model_fit\n",
    "\n",
    "# Define a helper function to calculate metrics\n",
    "def calculate_metrics(test_ARIMA, y_pred_ARIMA):\n",
    "    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n",
    "    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n",
    "    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n",
    "    return mae, mse, rmse, rmsle, r2\n",
    "\n",
    "dataTrain = dataTrain.sort_values(['family_encoded', 'date', 'type_encoded'])\n",
    "dataTrain_out = pd.DataFrame(columns=cols_out)\n",
    "metrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n",
    "\n",
    "def process_family(family):\n",
    "    print(family)\n",
    "    dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family]\n",
    "    dataTrainFamily = dataTrainFamily.iloc[int(84875):]#[29229395:-1]\n",
    "\n",
    "    last_month = dataTrainFamily['date'].max().to_period('M')\n",
    "    start_date = last_month.start_time + pd.DateOffset(days=1)\n",
    "    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n",
    "    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n",
    "    \n",
    "    series = train_.copy()\n",
    "    series.set_index('date', inplace=True)\n",
    "    series.index = series.index.to_period('M')\n",
    "    \n",
    "    test_inner = test_.copy()\n",
    "    test_inner.set_index('date', inplace=True)\n",
    "    test_inner.index = test_inner.index.to_period('M')\n",
    "    \n",
    "    test_exog = test_inner.copy()\n",
    "    test_exog = test_exog.drop(['test/train', 'sales'], axis=1)\n",
    "    exogenous = series.drop(['test/train', 'sales'], axis=1)\n",
    "    \n",
    "    test_ARIMA = test_inner['sales']\n",
    "    \n",
    "    pdqm = [(1, 0, 1, 12)]\n",
    "    \n",
    "    for so in pdqm:\n",
    "        exogenous['dcoilwtico_interpolated'] = exogenous['dcoilwtico_interpolated'].astype('int8')\n",
    "        exogenous['family_encoded'] = pd.Categorical(exogenous['family_encoded'])\n",
    "        exogenous['type_encoded'] = pd.Categorical(exogenous['type_encoded'])\n",
    "        exogenous['month'] = pd.Categorical(exogenous['month'])\n",
    "        exogenous['day_of_month'] = pd.Categorical(exogenous['day_of_month'])\n",
    "        exogenous['week_of_year'] = pd.Categorical(exogenous['week_of_year'])\n",
    "        exogenous['day_of_week'] = pd.Categorical(exogenous['day_of_week'])\n",
    "        \n",
    "        series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n",
    "        test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n",
    "        \n",
    "        test_exog['dcoilwtico_interpolated'] = test_exog['dcoilwtico_interpolated'].astype('int8')\n",
    "        test_exog['family_encoded'] = pd.Categorical(test_exog['family_encoded'])\n",
    "        test_exog['type_encoded'] = pd.Categorical(test_exog['type_encoded'])\n",
    "        test_exog['month'] = pd.Categorical(test_exog['month'])\n",
    "        test_exog['day_of_month'] = pd.Categorical(test_exog['day_of_month'])\n",
    "        test_exog['week_of_year'] = pd.Categorical(test_exog['week_of_year'])\n",
    "        test_exog['day_of_week'] = pd.Categorical(test_exog['day_of_week'])\n",
    "        \n",
    "        memory_bytes = exogenous.memory_usage(deep=True).sum()\n",
    "        memory_gb = memory_bytes / (1024 ** 3)\n",
    "        print(f\"Size of data: {memory_gb:.2f} GB\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create a pool of processes\n",
    "        pool = mp.Pool()\n",
    "        \n",
    "        # Fit SARIMAX models in parallel\n",
    "        results = [pool.apply_async(fit_sarimax, args=(series['sales'], exogenous, so)) for _ in range(8)]\n",
    "        \n",
    "        # Get the fitted models\n",
    "        model_fits = [result.get() for result in results]\n",
    "        \n",
    "        # Close the pool of processes\n",
    "        pool.close()\n",
    "        \n",
    "        # Wait for all processes to finish\n",
    "        pool.join()\n",
    "        \n",
    "        # Calculate the fitting time\n",
    "        fitting_time = time.time() - start_time\n",
    "        print(\"Fitting time:\", fitting_time, \"seconds\")\n",
    "        \n",
    "        for i, model_fit in enumerate(model_fits):\n",
    "            y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog=test_exog)\n",
    "            test_['y_pred_raw'] = y_pred_ARIMA.values\n",
    "            y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n",
    "            \n",
    "            if (y_pred_ARIMA < 0).any():\n",
    "                rmsle = 9999999\n",
    "            else:\n",
    "                rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n",
    "            \n",
    "            mae, mse, rmse, rmsle, r2 = calculate_metrics(test_ARIMA, y_pred_ARIMA)\n",
    "            \n",
    "            print(f'MAE: {mae:.2f}')\n",
    "            print(f'MSE: {mse:.2f}')\n",
    "            print(f'RMSE: {rmse:.2f}')\n",
    "            print(f'RMSLE: {rmsle:.2f}')\n",
    "            print(f'R2 score: {r2:.2f}')\n",
    "            \n",
    "            test_['y_pred'] = y_pred_ARIMA.values\n",
    "            dataTrain_out = dataTrain_out.append(test_)\n",
    "            metrics_df = metrics_df.append({'Model': 'SARIMAX', 'family_encoded': family, 'cols': list(train_.columns), \n",
    "                                            'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'R2': r2}, ignore_index=True)\n",
    "            \n",
    "            print('rolling mean RMSLE', metrics_df['RMSLE'].mean())\n",
    "            del y_pred_ARIMA, test_exog, series, exogenous\n",
    "            gc.collect()\n",
    "\n",
    "# Use multiprocessing to parallelize the family-wise processing\n",
    "pool = mp.Pool()\n",
    "dataTrain = dataTrain[dataTrain['family_encoded'] == 12 ]\n",
    "pool.map(process_family, dataTrain['family_encoded'].unique())\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "dataTrain_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import time\n",
    "\n",
    "\n",
    "#tscv = TimeSeriesSplit(2, test_size=250)\n",
    "#print(tscv)\n",
    "\n",
    "# Find the last month in the 'date' column\n",
    "last_month = dataTrain['date'].max().to_period('M')\n",
    "\n",
    "# Calculate the start date for the test data\n",
    "start_date = last_month.start_time + pd.DateOffset(days=1)\n",
    "\n",
    "# Filter the DataFrame based on the dynamic timeframe\n",
    "train_ = dataTrain[dataTrain['date'] < start_date]\n",
    "test_ = dataTrain[dataTrain['date'] >= start_date]\n",
    "\n",
    "series = train_.copy()\n",
    "series.set_index('date', inplace=True)\n",
    "series\n",
    "series.index = series.index.to_period('M')\n",
    "\n",
    "\n",
    "test_inner = test_.copy()\n",
    "test_inner.set_index('date', inplace=True)\n",
    "test_inner\n",
    "test_inner.index = test_inner.index.to_period('M')\n",
    "\n",
    "test_exog = test_inner.copy()\n",
    "test_exog = test_exog.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "print('test_inner', test_inner.shape)\n",
    "print('test_exog', test_exog.shape)\n",
    "\n",
    "test_ARIMA = test_inner['sales']\n",
    "\n",
    "print('test_inner', test_inner.shape)\n",
    "print('test_exog', test_exog.shape)\n",
    "\n",
    "exogenous = series.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "#(p, d, q, m)\n",
    "old_train = 0\n",
    "old_test = 0\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data['previous_sales'] = scaler.fit_transform(data['previous_sales'].values.reshape(-1, 1))\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = data[['category', 'location', 'holiday', 'previous_sales']].values\n",
    "y = data['sales'].values\n",
    "\n",
    "# Step 2: Splitting the Data\n",
    "# Assuming you have already split your data into train and validation sets\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# Step 3: Create Input Sequences\n",
    "# Assuming you have already created input sequences for your LSTM model\n",
    "\n",
    "# Create input sequences with a specific time window\n",
    "time_window = 10\n",
    "\n",
    "def create_sequences(X, y, time_window):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_window):\n",
    "        X_seq.append(X[i : i+time_window])\n",
    "        y_seq.append(y[i + time_window])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_window)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, time_window)\n",
    "\n",
    "# Step 4: Design and Train the LSTM Model\n",
    "\n",
    "# Design the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(time_window, X_train_seq.shape[2])))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, validation_data=(X_val_seq, y_val_seq))\n",
    "\n",
    "# Step 5: Evaluate and Fine-Tune the Model\n",
    "# Assuming you have already evaluated and fine-tuned your model\n",
    "\n",
    "# Evaluation on validation set\n",
    "val_loss = model.evaluate(X_val_seq, y_val_seq)\n",
    "print(\"Validation Loss:\", val_loss)\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "# Assuming you have new data for which you want to make predictions\n",
    "\n",
    "# Assuming 'new_data' contains the new data with the same columns as the training data\n",
    "new_data['category'] = label_encoder.transform(new_data['category'])\n",
    "new_data['location'] = label_encoder.transform(new_data['location'])\n",
    "new_data['holiday'] = label_encoder.transform(new_data['holiday'])\n",
    "new_data['previous_sales'] = scaler.transform(new_data['previous_sales'].values.reshape(-1, 1))\n",
    "\n",
    "# Create input sequence for prediction\n",
    "X_pred_seq, _ = create_sequences(new_data[['category', 'location', 'holiday', 'previous_sales']].values, [], time_window)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_pred_seq)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "\n",
    "\n",
    "for so in pdqm:\n",
    "    #for i, (train_index, test_index) in enumerate(tscv.split(train_.copy())):\n",
    "    print(f\"pdqm {so}:\")\n",
    "    #print(f\"  Train: index={train_index[0]}\", f\"  Train: index={train_index[-1]}\")\n",
    "    #print(f\"  Test:  index={test_index[0]}\", f\"  Test:  index={test_index[-1]}\")\n",
    "\n",
    "    print('series', series.shape)\n",
    "    print('test_exog', test_exog.shape)\n",
    "\n",
    "    # # fit model\n",
    "    #exogenous = series.copy()\n",
    "    #series = series['sales']\n",
    "    #exogenous = exogenous.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    #model = ARIMA(series, order=(1,0,4), exog=exogenous )\n",
    "    model = SARIMAX(series['sales'], order=(1, 0, 4), exog=exogenous, seasonal_order=(so))\n",
    "    model_fit = model.fit()\n",
    "\n",
    "    # Calculate the training time\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training time: {str(int(training_time))} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog= test_exog)\n",
    "\n",
    "    y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n",
    "\n",
    "    #test_['y_pred'] = y_pred_ARIMA\n",
    "\n",
    "    if (y_pred_ARIMA < 0).any():\n",
    "        rmsle = 9999999\n",
    "\n",
    "    else:    \n",
    "        #print((test_ARIMA.shape),(y_pred_ARIMA.shape))\n",
    "        #print(type(test_ARIMA),type(y_pred_ARIMA.shape))\n",
    "\n",
    "        rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n",
    "\n",
    "    # Evaluate the model\n",
    "    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n",
    "    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #rmsle = np.sqrt(mean_squared_log_error(y_test_transformed, y_pred_transformed))\n",
    "\n",
    "    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n",
    "\n",
    "    print(f'MAE: {mae:.2f}')\n",
    "    print(f'MSE: {mse:.2f}')\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    print(f'RMSLE: {rmsle:.2f}')\n",
    "    print(f'R2 score: {r2:.2f}')\n",
    "\n",
    "    #del \n",
    "    \n",
    "\n",
    "    metrics_df = metrics_df.append({'Model': 'SARIMAX', 'cols': list(train_loop.columns),  'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'R2': r2}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.sort_values(by='RMSLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(2, test_size=250)\n",
    "print(tscv)\n",
    "\n",
    "train_loop = train_.copy()\n",
    "test_loop = test.copy()\n",
    "\n",
    "old_train = 0\n",
    "old_test = 0\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(train_.copy())):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index[0]}\", f\"  Train: index={train_index[-1]}\")\n",
    "    print(f\"  Test:  index={test_index[0]}\", f\"  Test:  index={test_index[-1]}\")\n",
    "\n",
    "    series = train_loop.iloc[train_index[0]:train_index[-1]].copy()\n",
    "    series.shape\n",
    "    series.set_index('date', inplace=True)\n",
    "    series\n",
    "    series.index = series.index.to_period('M')\n",
    "    # # fit model\n",
    "    exogenous = series.copy()\n",
    "    series = series['sales']\n",
    "    exogenous = exogenous.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "\n",
    "    model = ARIMA(series, order=(1,0,4), exog=exogenous )\n",
    "    model_fit = model.fit()\n",
    "    # summary of fit model\n",
    "    #print(model_fit.summary())\n",
    "\n",
    "    test_inner = train_loop.iloc[test_index[0]:test_index[-1]].copy()\n",
    "    test_inner.set_index('date', inplace=True)\n",
    "    test_inner\n",
    "    test_inner.index = test_inner.index.to_period('M')\n",
    "\n",
    "    test_exog = test_inner.copy()\n",
    "    test_exog = test_exog.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "    print('test_inner', test_inner.shape)\n",
    "    print('test_exog', test_exog.shape)\n",
    "\n",
    "    test_ARIMA = test_inner['sales']\n",
    "\n",
    "    print('test_inner', test_inner.shape)\n",
    "    print('test_exog', test_exog.shape)\n",
    "\n",
    "    y_pred_ARIMA = model_fit.forecast(steps=len(test_ARIMA.copy()), exog= test_exog)\n",
    "\n",
    "    y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n",
    "\n",
    "    #test_['y_pred'] = y_pred_ARIMA\n",
    "\n",
    "    if (y_pred_ARIMA < 0).any():\n",
    "        rmsle = 9999999\n",
    "\n",
    "    else:    \n",
    "        #print((test_ARIMA.shape),(y_pred_ARIMA.shape))\n",
    "        #print(type(test_ARIMA),type(y_pred_ARIMA.shape))\n",
    "\n",
    "        rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n",
    "\n",
    "    # Evaluate the model\n",
    "    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n",
    "    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n",
    "    rmse = np.sqrt(mse)\n",
    "    #rmsle = np.sqrt(mean_squared_log_error(y_test_transformed, y_pred_transformed))\n",
    "\n",
    "    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n",
    "\n",
    "    print(f'MAE: {mae:.2f}')\n",
    "    print(f'MSE: {mse:.2f}')\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    print(f'RMSLE: {rmsle:.2f}')\n",
    "    print(f'R2 score: {r2:.2f}')\n",
    "    old_test = test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "tscv = TimeSeriesSplit(2, test_size=250)\n",
    "print(tscv)\n",
    "\n",
    "train_loop = train_.copy()\n",
    "test_loop = test.copy()\n",
    "#(p, d, q, m)\n",
    "old_train = 0\n",
    "old_test = 0\n",
    "\n",
    "pdqm = [(1, 0, 1, 12),\n",
    "(2, 1, 2, 12),\n",
    "(0, 1, 1, 7),\n",
    "(1, 1, 0, 4),\n",
    "(0, 0, 1, 24)]\n",
    "\n",
    "for so in pdqm:\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(train_.copy())):\n",
    "        print(f\"Fold {i}:\")\n",
    "        print(f\"  Train: index={train_index[0]}\", f\"  Train: index={train_index[-1]}\")\n",
    "        print(f\"  Test:  index={test_index[0]}\", f\"  Test:  index={test_index[-1]}\")\n",
    "\n",
    "        series = train_loop.iloc[train_index[0]:train_index[-1]].copy()\n",
    "        series.shape\n",
    "        series.set_index('date', inplace=True)\n",
    "        series\n",
    "        series.index = series.index.to_period('M')\n",
    "        # # fit model\n",
    "        exogenous = series.copy()\n",
    "        #series = series['sales']\n",
    "        #exogenous = exogenous.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "\n",
    "        #model = ARIMA(series, order=(1,0,4), exog=exogenous )\n",
    "        model = SARIMAX(series['sales'], order=(1, 0, 4), exog=series.drop(['test/train',  'sales'] , axis=1), seasonal_order=(so))\n",
    "\n",
    "        model_fit = model.fit()\n",
    "        # summary of fit model\n",
    "        #print(model_fit.summary())\n",
    "\n",
    "        test_inner = train_loop.iloc[test_index[0]:test_index[-1]].copy()\n",
    "        test_inner.set_index('date', inplace=True)\n",
    "        test_inner\n",
    "        test_inner.index = test_inner.index.to_period('M')\n",
    "\n",
    "        test_exog = test_inner.copy()\n",
    "        test_exog = test_exog.drop(['test/train',  'sales'] , axis=1)\n",
    "\n",
    "        print('test_inner', test_inner.shape)\n",
    "        print('test_exog', test_exog.shape)\n",
    "\n",
    "        test_ARIMA = test_inner['sales']\n",
    "\n",
    "        print('test_inner', test_inner.shape)\n",
    "        print('test_exog', test_exog.shape)\n",
    "\n",
    "        y_pred_ARIMA = model_fit.forecast(steps=len(test_ARIMA.copy()), exog= test_exog)\n",
    "\n",
    "        y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n",
    "\n",
    "        #test_['y_pred'] = y_pred_ARIMA\n",
    "\n",
    "        if (y_pred_ARIMA < 0).any():\n",
    "            rmsle = 9999999\n",
    "\n",
    "        else:    \n",
    "            #print((test_ARIMA.shape),(y_pred_ARIMA.shape))\n",
    "            #print(type(test_ARIMA),type(y_pred_ARIMA.shape))\n",
    "\n",
    "            rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n",
    "\n",
    "        # Evaluate the model\n",
    "        mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n",
    "        mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n",
    "        rmse = np.sqrt(mse)\n",
    "        #rmsle = np.sqrt(mean_squared_log_error(y_test_transformed, y_pred_transformed))\n",
    "\n",
    "        r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n",
    "\n",
    "        print(f'MAE: {mae:.2f}')\n",
    "        print(f'MSE: {mse:.2f}')\n",
    "        print(f'RMSE: {rmse:.2f}')\n",
    "        print(f'RMSLE: {rmsle:.2f}')\n",
    "        print(f'R2 score: {r2:.2f}')\n",
    "\n",
    "        #del \n",
    "        \n",
    "\n",
    "        metrics_df = metrics_df.append({'Model': 'SARIMAX', 'cols': list(train_loop.columns),  'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'R2': r2}, ignore_index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
