{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nbeats_keras\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:05:45.002589Z","iopub.execute_input":"2023-05-22T16:05:45.003720Z","iopub.status.idle":"2023-05-22T16:06:18.725955Z","shell.execute_reply.started":"2023-05-22T16:05:45.003682Z","shell.execute_reply":"2023-05-22T16:06:18.724378Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting nbeats_keras\n  Downloading nbeats_keras-1.8.0-py3-none-any.whl (7.3 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nbeats_keras) (1.5.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from nbeats_keras) (3.6.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nbeats_keras) (1.23.5)\nCollecting protobuf<=3.20\n  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting keract\n  Downloading keract-4.5.1-py3-none-any.whl (12 kB)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (from nbeats_keras) (2.11.0)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (from nbeats_keras) (2.11.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (1.0.7)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (21.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (9.5.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (1.4.4)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (4.39.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->nbeats_keras) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nbeats_keras) (2023.3)\nCollecting protobuf<=3.20\n  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (3.8.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (1.15.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (0.29.0)\nRequirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (2.11.2)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (0.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (1.6.3)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (4.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (1.16.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (16.0.0)\nRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (2.11.0)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (23.3.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (1.53.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (1.4.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (3.3.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow->nbeats_keras) (2.2.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->nbeats_keras) (0.40.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (3.4.3)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (2.2.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (2.28.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (0.6.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (2.17.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (1.8.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (5.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (0.2.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (1.3.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (1.26.15)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (2.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->nbeats_keras) (3.2.2)\nInstalling collected packages: keract, protobuf, nbeats_keras\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nonnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\nkfp 1.8.20 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\nkfp 1.8.20 requires PyYAML<6,>=5.3, but you have pyyaml 6.0 which is incompatible.\ngcsfs 2023.3.0 requires fsspec==2023.3.0, but you have fsspec 2023.4.0 which is incompatible.\nbeatrix-jupyterlab 2023.46.184821 requires jupyter-server~=1.16, but you have jupyter-server 2.5.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keract-4.5.1 nbeats_keras-1.8.0 protobuf-3.19.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# BASE\n# ------------------------------------------------------\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport warnings\nfrom pathlib import Path\n#!pip install xgboost\nfrom tqdm import tqdm\n\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn.linear_model  import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nimport xgboost as xgb\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\npd.set_option('display.max_colwidth', None)\nimport statsmodels.api as sm\nimport importlib.util\n\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\n\nimport numpy as np\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom nbeats_keras.model import NBeatsNet\nimport tensorflow as tf\nfrom keras import backend as K\n\n\n\nfrom pandas import datetime\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom matplotlib import pyplot\n\n# DATA VISUALIZATION\n# ------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# CONFIGURATIONS\n# ------------------------------------------------------\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format\nwarnings.filterwarnings('ignore')\nprint('config')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:06:20.460364Z","iopub.execute_input":"2023-05-22T16:06:20.460988Z","iopub.status.idle":"2023-05-22T16:06:20.477852Z","shell.execute_reply.started":"2023-05-22T16:06:20.460949Z","shell.execute_reply":"2023-05-22T16:06:20.476425Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"config\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport importlib.util\n#exit() \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename == 'data_setup_kaggle.py':\n            print('Found data_setup_kaggle.py')\n            module_name = 'data_setup_kaggle'\n            module_path = os.path.join(dirname, filename)\n            \n            spec = importlib.util.spec_from_file_location(module_name, module_path)\n            data_setup = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(data_setup)\n            \n            # Execute a function from the imported module and assign its result to 'd'\n            d = data_setup.get_data()\n            \n            # Now you can use the imported module\n            # ...\n            \n            \n# Compile the model\ndef rmsle_loss(y_true, y_pred):\n\n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = K.square(y_true_log - y_pred_log)\n    mean_squared_diff = K.mean(squared_diff, axis=-1)\n    rmsle = K.sqrt(mean_squared_diff)\n    return rmsle","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:06:20.481200Z","iopub.execute_input":"2023-05-22T16:06:20.482440Z","iopub.status.idle":"2023-05-22T16:07:29.039087Z","shell.execute_reply.started":"2023-05-22T16:06:20.482400Z","shell.execute_reply":"2023-05-22T16:07:29.037422Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found data_setup_kaggle.py\nParent directory: /kaggle\n(3029400, 81)\ncomplete\n","output_type":"stream"}]},{"cell_type":"code","source":"d.head()\n#d = d[d['date'].dt.year > 2016]\nd.head()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:29.040892Z","iopub.execute_input":"2023-05-22T16:07:29.041411Z","iopub.status.idle":"2023-05-22T16:07:29.117062Z","shell.execute_reply.started":"2023-05-22T16:07:29.041365Z","shell.execute_reply":"2023-05-22T16:07:29.115776Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   id       date  store_nbr      family  sales  onpromotion test/train   city  \\\n0   0 2013-01-01          1  AUTOMOTIVE   0.00         0.00      train  Quito   \n1   1 2013-01-01          1   BABY CARE   0.00         0.00      train  Quito   \n2   2 2013-01-01          1      BEAUTY   0.00         0.00      train  Quito   \n3   3 2013-01-01          1   BEVERAGES   0.00         0.00      train  Quito   \n4   4 2013-01-01          1       BOOKS   0.00         0.00      train  Quito   \n\n       state type  cluster  events_Black_Friday  events_Cyber_Monday  \\\n0  Pichincha    D       13                    0                    0   \n1  Pichincha    D       13                    0                    0   \n2  Pichincha    D       13                    0                    0   \n3  Pichincha    D       13                    0                    0   \n4  Pichincha    D       13                    0                    0   \n\n   events_Dia_de_la_Madre  events_Futbol  events_Terremoto_Manabi  \\\n0                       0              0                        0   \n1                       0              0                        0   \n2                       0              0                        0   \n3                       0              0                        0   \n4                       0              0                        0   \n\n   holiday_national_binary  holiday_local_binary  holiday_regional_binary  \\\n0                        1                     0                        0   \n1                        1                     0                        0   \n2                        1                     0                        0   \n3                        1                     0                        0   \n4                        1                     0                        0   \n\n   national_independence  local_cantonizacio  local_fundacion  \\\n0                      0                   1                1   \n1                      0                   1                1   \n2                      0                   1                1   \n3                      0                   1                1   \n4                      0                   1                1   \n\n   local_independencia  holiday_national_Batalla_de_Pichincha  \\\n0                    1                                      0   \n1                    1                                      0   \n2                    1                                      0   \n3                    1                                      0   \n4                    1                                      0   \n\n   holiday_national_Carnaval  holiday_national_Dia_de_Difuntos  \\\n0                          0                                 0   \n1                          0                                 0   \n2                          0                                 0   \n3                          0                                 0   \n4                          0                                 0   \n\n   holiday_national_Dia_de_la_Madre  holiday_national_Dia_del_Trabajo  \\\n0                                 0                                 0   \n1                                 0                                 0   \n2                                 0                                 0   \n3                                 0                                 0   \n4                                 0                                 0   \n\n   holiday_national_Independencia_de_Cuenca  \\\n0                                         0   \n1                                         0   \n2                                         0   \n3                                         0   \n4                                         0   \n\n   holiday_national_Independencia_de_Guayaquil  holiday_national_Navidad  \\\n0                                            0                         0   \n1                                            0                         0   \n2                                            0                         0   \n3                                            0                         0   \n4                                            0                         0   \n\n   holiday_national_Primer_Grito_de_Independencia  \\\n0                                               0   \n1                                               0   \n2                                               0   \n3                                               0   \n4                                               0   \n\n   holiday_national_Primer_dia_del_ano  holiday_national_Viernes_Santo  \\\n0                                    1                               0   \n1                                    1                               0   \n2                                    1                               0   \n3                                    1                               0   \n4                                    1                               0   \n\n   holiday_regional_Provincializacion_Santa_Elena  \\\n0                                               0   \n1                                               0   \n2                                               0   \n3                                               0   \n4                                               0   \n\n   holiday_regional_Provincializacion_de_Cotopaxi  \\\n0                                               0   \n1                                               0   \n2                                               0   \n3                                               0   \n4                                               0   \n\n   holiday_regional_Provincializacion_de_Imbabura  \\\n0                                               0   \n1                                               0   \n2                                               0   \n3                                               0   \n4                                               0   \n\n   holiday_regional_Provincializacion_de_Santo_Domingo  \\\n0                                                    0   \n1                                                    0   \n2                                                    0   \n3                                                    0   \n4                                                    0   \n\n   holiday_local_Cantonizacion_de_Cayambe  \\\n0                                       0   \n1                                       0   \n2                                       0   \n3                                       0   \n4                                       0   \n\n   holiday_local_Cantonizacion_de_El_Carmen  \\\n0                                         0   \n1                                         0   \n2                                         0   \n3                                         0   \n4                                         0   \n\n   holiday_local_Cantonizacion_de_Guaranda  \\\n0                                        0   \n1                                        0   \n2                                        0   \n3                                        0   \n4                                        0   \n\n   holiday_local_Cantonizacion_de_Latacunga  \\\n0                                         0   \n1                                         0   \n2                                         0   \n3                                         0   \n4                                         0   \n\n   holiday_local_Cantonizacion_de_Libertad  \\\n0                                        0   \n1                                        0   \n2                                        0   \n3                                        0   \n4                                        0   \n\n   holiday_local_Cantonizacion_de_Quevedo  \\\n0                                       0   \n1                                       0   \n2                                       0   \n3                                       0   \n4                                       0   \n\n   holiday_local_Cantonizacion_de_Riobamba  \\\n0                                        0   \n1                                        0   \n2                                        0   \n3                                        0   \n4                                        0   \n\n   holiday_local_Cantonizacion_de_Salinas  \\\n0                                       0   \n1                                       0   \n2                                       0   \n3                                       0   \n4                                       0   \n\n   holiday_local_Cantonizacion_del_Puyo  holiday_local_Fundacion_de_Ambato  \\\n0                                     0                                  0   \n1                                     0                                  0   \n2                                     0                                  0   \n3                                     0                                  0   \n4                                     0                                  0   \n\n   holiday_local_Fundacion_de_Cuenca  holiday_local_Fundacion_de_Esmeraldas  \\\n0                                  0                                      0   \n1                                  0                                      0   \n2                                  0                                      0   \n3                                  0                                      0   \n4                                  0                                      0   \n\n   holiday_local_Fundacion_de_Guayaquil  holiday_local_Fundacion_de_Ibarra  \\\n0                                     0                                  0   \n1                                     0                                  0   \n2                                     0                                  0   \n3                                     0                                  0   \n4                                     0                                  0   \n\n   holiday_local_Fundacion_de_Loja  holiday_local_Fundacion_de_Machala  \\\n0                                0                                   0   \n1                                0                                   0   \n2                                0                                   0   \n3                                0                                   0   \n4                                0                                   0   \n\n   holiday_local_Fundacion_de_Manta  holiday_local_Fundacion_de_Quito  \\\n0                                 0                                 0   \n1                                 0                                 0   \n2                                 0                                 0   \n3                                 0                                 0   \n4                                 0                                 0   \n\n   holiday_local_Fundacion_de_Riobamba  \\\n0                                    0   \n1                                    0   \n2                                    0   \n3                                    0   \n4                                    0   \n\n   holiday_local_Fundacion_de_Santo_Domingo  \\\n0                                         0   \n1                                         0   \n2                                         0   \n3                                         0   \n4                                         0   \n\n   holiday_local_Independencia_de_Ambato  \\\n0                                      0   \n1                                      0   \n2                                      0   \n3                                      0   \n4                                      0   \n\n   holiday_local_Independencia_de_Guaranda  \\\n0                                        0   \n1                                        0   \n2                                        0   \n3                                        0   \n4                                        0   \n\n   holiday_local_Independencia_de_Latacunga  month  day_of_month  day_of_year  \\\n0                                         0      1             1            1   \n1                                         0      1             1            1   \n2                                         0      1             1            1   \n3                                         0      1             1            1   \n4                                         0      1             1            1   \n\n   week_of_month  week_of_year  day_of_week  year  is_wknd  quarter  \\\n0              1             1            2  2013        0        1   \n1              1             1            2  2013        0        1   \n2              1             1            2  2013        0        1   \n3              1             1            2  2013        0        1   \n4              1             1            2  2013        0        1   \n\n   is_month_start  is_month_end  is_quarter_start  is_quarter_end  \\\n0               1             0                 1               0   \n1               1             0                 1               0   \n2               1             0                 1               0   \n3               1             0                 1               0   \n4               1             0                 1               0   \n\n   is_year_start  is_year_end  season  workday  wageday  dcoilwtico  \\\n0              1            0       0        0        0         NaN   \n1              1            0       0        0        0         NaN   \n2              1            0       0        0        0         NaN   \n3              1            0       0        0        0         NaN   \n4              1            0       0        0        0         NaN   \n\n   dcoilwtico_interpolated  \n0                    93.14  \n1                    93.14  \n2                    93.14  \n3                    93.14  \n4                    93.14  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>date</th>\n      <th>store_nbr</th>\n      <th>family</th>\n      <th>sales</th>\n      <th>onpromotion</th>\n      <th>test/train</th>\n      <th>city</th>\n      <th>state</th>\n      <th>type</th>\n      <th>cluster</th>\n      <th>events_Black_Friday</th>\n      <th>events_Cyber_Monday</th>\n      <th>events_Dia_de_la_Madre</th>\n      <th>events_Futbol</th>\n      <th>events_Terremoto_Manabi</th>\n      <th>holiday_national_binary</th>\n      <th>holiday_local_binary</th>\n      <th>holiday_regional_binary</th>\n      <th>national_independence</th>\n      <th>local_cantonizacio</th>\n      <th>local_fundacion</th>\n      <th>local_independencia</th>\n      <th>holiday_national_Batalla_de_Pichincha</th>\n      <th>holiday_national_Carnaval</th>\n      <th>holiday_national_Dia_de_Difuntos</th>\n      <th>holiday_national_Dia_de_la_Madre</th>\n      <th>holiday_national_Dia_del_Trabajo</th>\n      <th>holiday_national_Independencia_de_Cuenca</th>\n      <th>holiday_national_Independencia_de_Guayaquil</th>\n      <th>holiday_national_Navidad</th>\n      <th>holiday_national_Primer_Grito_de_Independencia</th>\n      <th>holiday_national_Primer_dia_del_ano</th>\n      <th>holiday_national_Viernes_Santo</th>\n      <th>holiday_regional_Provincializacion_Santa_Elena</th>\n      <th>holiday_regional_Provincializacion_de_Cotopaxi</th>\n      <th>holiday_regional_Provincializacion_de_Imbabura</th>\n      <th>holiday_regional_Provincializacion_de_Santo_Domingo</th>\n      <th>holiday_local_Cantonizacion_de_Cayambe</th>\n      <th>holiday_local_Cantonizacion_de_El_Carmen</th>\n      <th>holiday_local_Cantonizacion_de_Guaranda</th>\n      <th>holiday_local_Cantonizacion_de_Latacunga</th>\n      <th>holiday_local_Cantonizacion_de_Libertad</th>\n      <th>holiday_local_Cantonizacion_de_Quevedo</th>\n      <th>holiday_local_Cantonizacion_de_Riobamba</th>\n      <th>holiday_local_Cantonizacion_de_Salinas</th>\n      <th>holiday_local_Cantonizacion_del_Puyo</th>\n      <th>holiday_local_Fundacion_de_Ambato</th>\n      <th>holiday_local_Fundacion_de_Cuenca</th>\n      <th>holiday_local_Fundacion_de_Esmeraldas</th>\n      <th>holiday_local_Fundacion_de_Guayaquil</th>\n      <th>holiday_local_Fundacion_de_Ibarra</th>\n      <th>holiday_local_Fundacion_de_Loja</th>\n      <th>holiday_local_Fundacion_de_Machala</th>\n      <th>holiday_local_Fundacion_de_Manta</th>\n      <th>holiday_local_Fundacion_de_Quito</th>\n      <th>holiday_local_Fundacion_de_Riobamba</th>\n      <th>holiday_local_Fundacion_de_Santo_Domingo</th>\n      <th>holiday_local_Independencia_de_Ambato</th>\n      <th>holiday_local_Independencia_de_Guaranda</th>\n      <th>holiday_local_Independencia_de_Latacunga</th>\n      <th>month</th>\n      <th>day_of_month</th>\n      <th>day_of_year</th>\n      <th>week_of_month</th>\n      <th>week_of_year</th>\n      <th>day_of_week</th>\n      <th>year</th>\n      <th>is_wknd</th>\n      <th>quarter</th>\n      <th>is_month_start</th>\n      <th>is_month_end</th>\n      <th>is_quarter_start</th>\n      <th>is_quarter_end</th>\n      <th>is_year_start</th>\n      <th>is_year_end</th>\n      <th>season</th>\n      <th>workday</th>\n      <th>wageday</th>\n      <th>dcoilwtico</th>\n      <th>dcoilwtico_interpolated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>AUTOMOTIVE</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>train</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>93.14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>BABY CARE</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>train</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>93.14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>BEAUTY</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>train</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>93.14</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>BEVERAGES</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>train</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>93.14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>BOOKS</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>train</td>\n      <td>Quito</td>\n      <td>Pichincha</td>\n      <td>D</td>\n      <td>13</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2013</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>93.14</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"metrics_df = pd.DataFrame(columns=['Model', 'family_encoded', 'cols', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2'])\n\n# preform test train split\ncols = ['test/train' ,'sales', 'store_nbr', 'type', 'family', 'date', 'month','day_of_month', 'week_of_year','day_of_week', 'dcoilwtico_interpolated']\n#cols = ['test/train' ,'sales', 'family', 'date', 'month','day_of_month', 'week_of_year','day_of_week', 'type', 'dcoilwtico_interpolated']\n\ndataTrainTest = d[cols]\n\ndataTrain = dataTrainTest[dataTrainTest['test/train'] ==  'train'].reset_index(drop=True)\ndataTest = dataTrainTest[dataTrainTest['test/train'] ==  'test'].reset_index(drop=True)\n\ndataTrain\n\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndataTrain[\"family_encoded\"] = encoder.fit_transform(dataTrain[\"family\"])\ndataTrain.drop('family', axis=1, inplace=True)\n\ndataTrain[\"type\"]\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndataTrain[\"type_encoded\"] = encoder.fit_transform(dataTrain[\"type\"])\ndataTrain.drop('type', axis=1, inplace=True)\n\ntrain_ = (dataTrain)\ntest_ = (dataTest)\n\n#train_, test_ = train_test_split(dataTrain.dropna().reset_index(drop=True), test_size=0.2, random_state=42)\n\ndel d\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:29.118633Z","iopub.execute_input":"2023-05-22T16:07:29.119083Z","iopub.status.idle":"2023-05-22T16:07:32.133132Z","shell.execute_reply.started":"2023-05-22T16:07:29.119049Z","shell.execute_reply":"2023-05-22T16:07:32.131694Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}]},{"cell_type":"code","source":"#dataTrain = dataTrain[dataTrain['family_encoded'] == 27 ]\ndataTrain","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:32.134483Z","iopub.execute_input":"2023-05-22T16:07:32.134870Z","iopub.status.idle":"2023-05-22T16:07:32.162836Z","shell.execute_reply.started":"2023-05-22T16:07:32.134838Z","shell.execute_reply":"2023-05-22T16:07:32.161585Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        test/train   sales  store_nbr       date  month  day_of_month  \\\n0            train    0.00          1 2013-01-01      1             1   \n1            train    0.00          1 2013-01-01      1             1   \n2            train    0.00          1 2013-01-01      1             1   \n3            train    0.00          1 2013-01-01      1             1   \n4            train    0.00          1 2013-01-01      1             1   \n...            ...     ...        ...        ...    ...           ...   \n3000883      train  438.13          9 2017-08-15      8            15   \n3000884      train  154.55          9 2017-08-15      8            15   \n3000885      train 2419.73          9 2017-08-15      8            15   \n3000886      train  121.00          9 2017-08-15      8            15   \n3000887      train   16.00          9 2017-08-15      8            15   \n\n         week_of_year  day_of_week  dcoilwtico_interpolated  family_encoded  \\\n0                   1            2                    93.14               0   \n1                   1            2                    93.14               1   \n2                   1            2                    93.14               2   \n3                   1            2                    93.14               3   \n4                   1            2                    93.14               4   \n...               ...          ...                      ...             ...   \n3000883            33            2                    47.32              28   \n3000884            33            2                    47.32              29   \n3000885            33            2                    47.32              30   \n3000886            33            2                    47.32              31   \n3000887            33            2                    47.32              32   \n\n         type_encoded  \n0                   3  \n1                   3  \n2                   3  \n3                   3  \n4                   3  \n...               ...  \n3000883             1  \n3000884             1  \n3000885             1  \n3000886             1  \n3000887             1  \n\n[3000888 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test/train</th>\n      <th>sales</th>\n      <th>store_nbr</th>\n      <th>date</th>\n      <th>month</th>\n      <th>day_of_month</th>\n      <th>week_of_year</th>\n      <th>day_of_week</th>\n      <th>dcoilwtico_interpolated</th>\n      <th>family_encoded</th>\n      <th>type_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>93.14</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>93.14</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>93.14</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>93.14</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>2013-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>93.14</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3000883</th>\n      <td>train</td>\n      <td>438.13</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>28</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3000884</th>\n      <td>train</td>\n      <td>154.55</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>29</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3000885</th>\n      <td>train</td>\n      <td>2419.73</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>30</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3000886</th>\n      <td>train</td>\n      <td>121.00</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>31</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3000887</th>\n      <td>train</td>\n      <td>16.00</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3000888 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"cols_out = list(dataTrain.columns)\ncols_out.append('y_pred')","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:32.165306Z","iopub.execute_input":"2023-05-22T16:07:32.165766Z","iopub.status.idle":"2023-05-22T16:07:32.172426Z","shell.execute_reply.started":"2023-05-22T16:07:32.165717Z","shell.execute_reply":"2023-05-22T16:07:32.170797Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cols_out","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:32.174221Z","iopub.execute_input":"2023-05-22T16:07:32.174932Z","iopub.status.idle":"2023-05-22T16:07:32.192779Z","shell.execute_reply.started":"2023-05-22T16:07:32.174889Z","shell.execute_reply":"2023-05-22T16:07:32.191253Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['test/train',\n 'sales',\n 'store_nbr',\n 'date',\n 'month',\n 'day_of_month',\n 'week_of_year',\n 'day_of_week',\n 'dcoilwtico_interpolated',\n 'family_encoded',\n 'type_encoded',\n 'y_pred']"},"metadata":{}}]},{"cell_type":"code","source":"#dataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\ndataTrain['family_encoded'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:32.198138Z","iopub.execute_input":"2023-05-22T16:07:32.198588Z","iopub.status.idle":"2023-05-22T16:07:32.234656Z","shell.execute_reply.started":"2023-05-22T16:07:32.198553Z","shell.execute_reply":"2023-05-22T16:07:32.233337Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"33"},"metadata":{}}]},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport sys\n\n# Define a function to fit SARIMAX model\ndef fit_sarimax(data, exogenous, so):\n    model = SARIMAX(data, order=(1, 0, 4), exog=exogenous, seasonal_order=so)\n    model_fit = model.fit(disp=False, maxiter=2000, method='nm')\n    return model_fit\n\n# Define a helper function to calculate metrics\ndef calculate_metrics(test_ARIMA, y_pred_ARIMA):\n    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n    y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n    rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n    return mae, mse, rmse, rmsle, r2\n\ndataTrain = dataTrain.sort_values(['family_encoded', 'date', 'type_encoded'])\ndataTrain_out = pd.DataFrame(columns=cols_out)\nmetrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n\n#def process_family(family):\n#    global dataTrain\n# Define the process_family function\ndef process_family(family):\n    global dataTrain_out, metrics_df, dataTrain\n    print(family)\n    sys.stdout.flush()\n    dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family]\n    last_month = dataTrainFamily['date'].max().to_period('M')\n    start_date = last_month.start_time + pd.DateOffset(days=1)\n    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n\n    series = train_.copy(deep=True)\n    series.set_index('date', inplace=True)\n    series.index = series.index.to_period('M')\n\n    test_inner = test_.copy(deep=True)\n    test_inner.set_index('date', inplace=True)\n    test_inner.index = test_inner.index.to_period('M')\n\n    test_exog = test_inner.drop(['test/train', 'sales'], axis=1)\n    exogenous = series.drop(['test/train', 'sales'], axis=1)\n\n    test_ARIMA = test_inner['sales']\n\n    pdqm = [(1, 0, 1, 12)]\n\n    for so in pdqm:\n        exogenous['dcoilwtico_interpolated'] = exogenous['dcoilwtico_interpolated'].astype('int8')\n        #exogenous['family_encoded'] = pd.Categorical(exogenous['family_encoded'])\n        exogenous['type_encoded'] = pd.Categorical(exogenous['type_encoded'])\n        #exogenous['month'] = pd.Categorical(exogenous['month'])\n        #exogenous['day_of_month'] = pd.Categorical(exogenous['day_of_month'])\n        #exogenous['week_of_year'] = pd.Categorical(exogenous['week_of_year'])\n        #exogenous['day_of_week'] = pd.Categorical(exogenous['day_of_week'])\n\n        series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n        test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n\n        test_exog['dcoilwtico_interpolated'] = test_exog['dcoilwtico_interpolated'].astype('int8')\n        #test_exog['family_encoded'] = pd.Categorical(test_exog['family_encoded'])\n        test_exog['type_encoded'] = pd.Categorical(test_exog['type_encoded'])\n        #test_exog['month'] = pd.Categorical(test_exog['month'])\n        #test_exog['day_of_month'] = pd.Categorical(test_exog['day_of_month'])\n        #test_exog['week_of_year'] = pd.Categorical(test_exog['week_of_year'])\n        #test_exog['day_of_week'] = pd.Categorical(test_exog['day_of_week'])\n\n       # try:\n        start_time = time.time()\n        #model = fit_sarimax(series['sales'], exogenous, so)\n        #model_fit = model.fit()\n\n        model = SARIMAX(series['sales'], order=(1, 0, 4), seasonal_order=(so))\n        model_fit = model.fit(disp=False)#, method='nm', max_iter=1000)\n\n        #_pred_ARIMA = model_fit.predict(start=test_inner.index[0].start_time, end=test_inner.index[-1].end_time, exog=test_exog)\n\n\n                # Calculate the fitting time\n        fitting_time = time.time() - start_time\n        # Print the fitting time\n        print(\"Fitting time:\", fitting_time, \"seconds\")\n        sys.stdout.flush()\n\n        y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog= test_exog)\n\n\n        y_pred_ARIMA = np.round(y_pred_ARIMA).astype(int)\n        mae, mse, rmse, rmsle, r2 = calculate_metrics(test_ARIMA, y_pred_ARIMA)\n        sys.stdout.flush()\n\n        result = pd.DataFrame([[str(so), family, exogenous.columns.tolist(), mae, mse, rmse, rmsle, r2]],\n                              columns=['Model', 'family_encoded', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n        #metrics_df = metrics_df.append(result, ignore_index=True)\n        \n        test_['y_pred'] = y_pred_ARIMA.values\n        dataTrain_out = pd.concat([dataTrain_out, test_], ignore_index=True)\n        print('Faamily mean ', result['RMSLE'].mean())\n\n        #print('rmsle',rmsle)\n        #print('test_[y_pred]',test_['y_pred'])\n        #print('dataTrain_out[y_pred]',dataTrain_out['y_pred'])\n\n        sys.stdout.flush()\n\n        del model_fit, y_pred_ARIMA\n        gc.collect()\n        #except Exception as e:\n         #   print(e)\n\n    return (dataTrain_out)\n\n\n\n\n\n# Use joblib to parallelize the family-wise processing\n#dataTrain_result = Parallel(n_jobs=4, prefer='processes')(delayed(process_family)(family) for family in [12, 13, 14, 15])\ndataTrain_result = Parallel(n_jobs=4, prefer='processes')(delayed(process_family)(family) for family in dataTrain['family_encoded'].unique())\n\nfor data in dataTrain_result:\n    dataTrain_out = pd.concat([dataTrain_out, data], ignore_index=True)\n\n\n    \n#Parallel(n_jobs=4, prefer='processes')(delayed(process_family)(family) for family in dataTrain['family_encoded'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:07:32.236505Z","iopub.execute_input":"2023-05-22T16:07:32.236932Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"0\n1\n2\n3\n","output_type":"stream"}]},{"cell_type":"code","source":"group = dataTrain_out.groupby('family_encoded')\n\nfor g in group:\n    df = g[1]  # Access the DataFrame from the tuple using index 1\n    #print(df)\n    mae, mse, rmse, rmsle, r2 = calculate_metrics((df['sales']).to_numpy(), (df['y_pred']).to_numpy())\n    dataTrain_out.loc[dataTrain_out['family_encoded'] ==df['family_encoded'].iloc[0], ['mae', 'mse', 'rmse', 'rmsle', 'r2']] = mae, mse, rmse, rmsle, r2\n\n    print('Family: {}, MAE: {:.2f}, MSE: {:.2f}, RMSE: {:.2f}, RMSLE: {:.2f}, R2: {:.2f}'.format(df['family_encoded'].iloc[0], mae, mse, rmse, rmsle, r2))\n\ndataTrain_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out['%_diff'] = dataTrain_out['sales'] / dataTrain_out['y_pred']","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out['%_diff'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out['%_diff'].group_by('family_encoded').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out\n#metrics_df\n#result","metadata":{"execution":{"iopub.status.busy":"2023-05-22T14:32:53.481190Z","iopub.execute_input":"2023-05-22T14:32:53.481593Z","iopub.status.idle":"2023-05-22T14:32:53.504279Z","shell.execute_reply.started":"2023-05-22T14:32:53.481564Z","shell.execute_reply":"2023-05-22T14:32:53.503000Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"     test/train    sales store_nbr       date month day_of_month week_of_year  \\\n0         train 11156.00        44 2017-08-02     8            2           31   \n1         train 12494.00        45 2017-08-02     8            2           31   \n2         train 10701.00        46 2017-08-02     8            2           31   \n3         train 12263.00        47 2017-08-02     8            2           31   \n4         train  9945.00        48 2017-08-02     8            2           31   \n...         ...      ...       ...        ...   ...          ...          ...   \n3019      train    40.00         8 2017-08-15     8           15           33   \n3020      train    38.00        28 2017-08-15     8           15           33   \n3021      train    33.00        29 2017-08-15     8           15           33   \n3022      train    32.00        36 2017-08-15     8           15           33   \n3023      train    27.00        43 2017-08-15     8           15           33   \n\n     day_of_week  dcoilwtico_interpolated family_encoded type_encoded y_pred  \n0              3                    49.27             12            0   9393  \n1              3                    49.27             12            0   9241  \n2              3                    49.27             12            0   8756  \n3              3                    49.27             12            0   7818  \n4              3                    49.27             12            0   7693  \n...          ...                      ...            ...          ...    ...  \n3019           2                    47.32             15            3     12  \n3020           2                    47.32             15            4      0  \n3021           2                    47.32             15            4      0  \n3022           2                    47.32             15            4      0  \n3023           2                    47.32             15            4      0  \n\n[3024 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test/train</th>\n      <th>sales</th>\n      <th>store_nbr</th>\n      <th>date</th>\n      <th>month</th>\n      <th>day_of_month</th>\n      <th>week_of_year</th>\n      <th>day_of_week</th>\n      <th>dcoilwtico_interpolated</th>\n      <th>family_encoded</th>\n      <th>type_encoded</th>\n      <th>y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train</td>\n      <td>11156.00</td>\n      <td>44</td>\n      <td>2017-08-02</td>\n      <td>8</td>\n      <td>2</td>\n      <td>31</td>\n      <td>3</td>\n      <td>49.27</td>\n      <td>12</td>\n      <td>0</td>\n      <td>9393</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train</td>\n      <td>12494.00</td>\n      <td>45</td>\n      <td>2017-08-02</td>\n      <td>8</td>\n      <td>2</td>\n      <td>31</td>\n      <td>3</td>\n      <td>49.27</td>\n      <td>12</td>\n      <td>0</td>\n      <td>9241</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train</td>\n      <td>10701.00</td>\n      <td>46</td>\n      <td>2017-08-02</td>\n      <td>8</td>\n      <td>2</td>\n      <td>31</td>\n      <td>3</td>\n      <td>49.27</td>\n      <td>12</td>\n      <td>0</td>\n      <td>8756</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train</td>\n      <td>12263.00</td>\n      <td>47</td>\n      <td>2017-08-02</td>\n      <td>8</td>\n      <td>2</td>\n      <td>31</td>\n      <td>3</td>\n      <td>49.27</td>\n      <td>12</td>\n      <td>0</td>\n      <td>7818</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train</td>\n      <td>9945.00</td>\n      <td>48</td>\n      <td>2017-08-02</td>\n      <td>8</td>\n      <td>2</td>\n      <td>31</td>\n      <td>3</td>\n      <td>49.27</td>\n      <td>12</td>\n      <td>0</td>\n      <td>7693</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3019</th>\n      <td>train</td>\n      <td>40.00</td>\n      <td>8</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>3</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3020</th>\n      <td>train</td>\n      <td>38.00</td>\n      <td>28</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3021</th>\n      <td>train</td>\n      <td>33.00</td>\n      <td>29</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3022</th>\n      <td>train</td>\n      <td>32.00</td>\n      <td>36</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3023</th>\n      <td>train</td>\n      <td>27.00</td>\n      <td>43</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3024 rows × 12 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from joblib import Parallel, delayed\n\n# Define a function to fit SARIMAX model\ndef fit_sarimax(data, exogenous, so):\n    model = SARIMAX(data, order=(1, 0, 4), exog=exogenous, seasonal_order=so)\n    model_fit = model.fit(disp=False)\n    return model_fit\n\n# Define a helper function to calculate metrics\ndef calculate_metrics(test_ARIMA, y_pred_ARIMA):\n    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n    rmse = np.sqrt(mse)\n    rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n    return mae, mse, rmse, rmsle, r2\n\ndataTrain = dataTrain.sort_values(['family_encoded', 'date', 'type_encoded'])\ndataTrain_out = pd.DataFrame(columns=cols_out)\nmetrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n\n#def process_family(family):\n#    global dataTrain\ndef process_family(family):\n    global dataTrain_out, metrics_df, dataTrain\n    print(family)\n    sys.stdout.flush()\n    dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family]\n    #dataTrainFamily = dataTrainFamily.iloc[int(84875):]#[29229395:-1]\n    \n    last_month = dataTrainFamily['date'].max().to_period('M')\n    start_date = last_month.start_time + pd.DateOffset(days=1)\n    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n    \n    series = train_.copy(deep=True)\n    series.set_index('date', inplace=True)\n    series.index = series.index.to_period('M')\n    \n    test_inner = test_.copy(deep=True)\n    test_inner.set_index('date', inplace=True)\n    test_inner.index = test_inner.index.to_period('M')\n    \n    test_exog = test_inner.copy(deep=True)\n    test_exog = test_exog.drop(['test/train', 'sales'], axis=1)\n    exogenous = series.drop(['test/train', 'sales'], axis=1)\n    \n    test_ARIMA = test_inner['sales']\n    \n    pdqm = [(1, 0, 1, 12)]\n    \n    for so in pdqm:\n        exogenous['dcoilwtico_interpolated'] = exogenous['dcoilwtico_interpolated'].astype('int8')\n        exogenous['family_encoded'] = pd.Categorical(exogenous['family_encoded'])\n        exogenous['type_encoded'] = pd.Categorical(exogenous['type_encoded'])\n        exogenous['month'] = pd.Categorical(exogenous['month'])\n        exogenous['day_of_month'] = pd.Categorical(exogenous['day_of_month'])\n        exogenous['week_of_year'] = pd.Categorical(exogenous['week_of_year'])\n        exogenous['day_of_week'] = pd.Categorical(exogenous['day_of_week'])\n        \n        series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n        test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n        \n        test_exog['dcoilwtico_interpolated'] = test_exog['dcoilwtico_interpolated'].astype('int8')\n        test_exog['family_encoded'] = pd.Categorical(test_exog['family_encoded'])\n        test_exog['type_encoded'] = pd.Categorical(test_exog['type_encoded'])\n        test_exog['month'] = pd.Categorical(test_exog['month'])\n        test_exog['day_of_month'] = pd.Categorical(test_exog['day_of_month'])\n        test_exog['week_of_year'] = pd.Categorical(test_exog['week_of_year'])\n        test_exog['day_of_week'] = pd.Categorical(test_exog['day_of_week'])\n        \n        memory_bytes = exogenous.memory_usage(deep=True).sum()\n        memory_gb = memory_bytes / (1024 ** 3)\n        print(f\"Size of data: {memory_gb:.2f} GB\")\n        \n        start_time = time.time()\n        \n        # Fit SARIMAX models in parallel using joblib\n\n        model = SARIMAX(series['sales'], order=(1, 0, 4), exog=exogenous, seasonal_order=(so))\n        model_fit = model.fit(disp=False)\n        del model\n        \n        # Calculate the fitting time\n        fitting_time = time.time() - start_time\n\n        # Print the fitting time\n        print(\"Fitting time:\", fitting_time, \"seconds\")\n\n\n        y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog= test_exog)\n\n        test_['y_pred_raw'] = y_pred_ARIMA.values\n\n        y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n\n\n        if (y_pred_ARIMA < 0).any():\n            rmsle = 9999999\n\n        else:    \n            print((test_ARIMA.shape),(y_pred_ARIMA.shape))\n            print(type(test_ARIMA),type(y_pred_ARIMA.shape))\n            rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n            sys.stdout.flush()\n\n\n        # Evaluate the model\n        mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n        mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n        rmse = np.sqrt(mse)\n\n        r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n\n        print(f'MAE: {mae:.2f}')\n        print(f'MSE: {mse:.2f}')\n        print(f'RMSE: {rmse:.2f}')\n        print(f'RMSLE: {rmsle:.2f}')\n        print(f'R2 score: {r2:.2f}')\n        sys.stdout.flush()\n\n        print(y_pred_ARIMA.values)\n        test_['y_pred'] = y_pred_ARIMA.values\n        \n        dataTrain_out = dataTrain_out.append(test_)\n\n        metrics_df = metrics_df.append({'Model': 'SARIMAX', 'family_encoded':family , 'cols': list(train_.columns),  'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'R2': r2}, ignore_index=True)\n        print('rolling mean RMSLE',metrics_df['RMSLE'].mean())\n        del y_pred_ARIMA, test_exog, series, exogenous\n        gc.collect()\n\n# Use joblib to parallelize the family-wise processing\nParallel(n_jobs=4, prefer='processes')(delayed(process_family)(family) for family in dataTrain['family_encoded'].unique())\n\ndataTrain_out\n\n# Use joblib to parallelize the family-wise processing\n#dataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\n#dataTrain\n#Parallel(n_jobs=4, prefer='processes')(delayed(process_family)(family) for family in dataTrain['family_encoded'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T13:34:00.630088Z","iopub.execute_input":"2023-05-19T13:34:00.630451Z","iopub.status.idle":"2023-05-19T13:35:24.189270Z","shell.execute_reply.started":"2023-05-19T13:34:00.630427Z","shell.execute_reply":"2023-05-19T13:35:24.187499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain['family_encoded'].nunique()","metadata":{"execution":{"iopub.status.busy":"2023-05-19T13:31:14.179229Z","iopub.execute_input":"2023-05-19T13:31:14.179629Z","iopub.status.idle":"2023-05-19T13:31:14.187003Z","shell.execute_reply.started":"2023-05-19T13:31:14.179598Z","shell.execute_reply":"2023-05-19T13:31:14.185946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T13:31:06.283953Z","iopub.execute_input":"2023-05-19T13:31:06.284292Z","iopub.status.idle":"2023-05-19T13:31:06.295425Z","shell.execute_reply.started":"2023-05-19T13:31:06.284269Z","shell.execute_reply":"2023-05-19T13:31:06.294667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing as mp\nglobal dataTrain\n\n# Define a function to fit SARIMAX model\ndef fit_sarimax(data, exogenous, so):\n    model = SARIMAX(data, order=(1, 0, 4), exog=exogenous, seasonal_order=so)\n    model_fit = model.fit(disp=False)\n    return model_fit\n\n# Define a helper function to calculate metrics\ndef calculate_metrics(test_ARIMA, y_pred_ARIMA):\n    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n    rmse = np.sqrt(mse)\n    rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n    return mae, mse, rmse, rmsle, r2\n\ndataTrain = dataTrain.sort_values(['family_encoded', 'date', 'type_encoded'])\ndataTrain_out = pd.DataFrame(columns=cols_out)\nmetrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n\ndef process_family(dataTrain):\n    for family in tqdm(dataTrain['family_encoded'].unique()):\n        print(family)\n        dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family]\n        dataTrainFamily = dataTrainFamily.iloc[int(84875):]#[29229395:-1]\n\n        last_month = dataTrainFamily['date'].max().to_period('M')\n        start_date = last_month.start_time + pd.DateOffset(days=1)\n        train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n        test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n\n        series = train_.copy()\n        series.set_index('date', inplace=True)\n        series.index = series.index.to_period('M')\n\n        test_inner = test_.copy()\n        test_inner.set_index('date', inplace=True)\n        test_inner.index = test_inner.index.to_period('M')\n\n        test_exog = test_inner.copy()\n        test_exog = test_exog.drop(['test/train', 'sales'], axis=1)\n        exogenous = series.drop(['test/train', 'sales'], axis=1)\n\n        test_ARIMA = test_inner['sales']\n\n        pdqm = [(1, 0, 1, 12)]\n\n        for so in pdqm:\n            exogenous['dcoilwtico_interpolated'] = exogenous['dcoilwtico_interpolated'].astype('int8')\n            exogenous['family_encoded'] = pd.Categorical(exogenous['family_encoded'])\n            exogenous['type_encoded'] = pd.Categorical(exogenous['type_encoded'])\n            exogenous['month'] = pd.Categorical(exogenous['month'])\n            exogenous['day_of_month'] = pd.Categorical(exogenous['day_of_month'])\n            exogenous['week_of_year'] = pd.Categorical(exogenous['week_of_year'])\n            exogenous['day_of_week'] = pd.Categorical(exogenous['day_of_week'])\n\n            series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n            test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n\n            test_exog['dcoilwtico_interpolated'] = test_exog['dcoilwtico_interpolated'].astype('int8')\n            test_exog['family_encoded'] = pd.Categorical(test_exog['family_encoded'])\n            test_exog['type_encoded'] = pd.Categorical(test_exog['type_encoded'])\n            test_exog['month'] = pd.Categorical(test_exog['month'])\n            test_exog['day_of_month'] = pd.Categorical(test_exog['day_of_month'])\n            test_exog['week_of_year'] = pd.Categorical(test_exog['week_of_year'])\n            test_exog['day_of_week'] = pd.Categorical(test_exog['day_of_week'])\n\n            memory_bytes = exogenous.memory_usage(deep=True).sum()\n            memory_gb = memory_bytes / (1024 ** 3)\n            print(f\"Size of data: {memory_gb:.2f} GB\")\n\n            start_time = time.time()\n\n            # Create a pool of processes\n            sys.stdout.flush()\n\n            pool = mp.Pool()\n\n            # Fit SARIMAX models in parallel\n            results = [pool.apply_async(fit_sarimax, args=(series['sales'], exogenous, so)) for _ in range(4)]\n\n            # Get the fitted models\n            model_fits = [result.get() for result in results]\n\n            # Close the pool of processes\n            pool.close()\n\n            # Wait for all processes to finish\n            pool.join()\n\n            # Calculate the fitting time\n            fitting_time = time.time() - start_time\n            print(\"Fitting time:\", fitting_time, \"seconds\")\n            sys.stdout.flush()\n\n            for i, model_fit in enumerate(model_fits):\n                y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog=test_exog)\n                test_['y_pred_raw'] = y_pred_ARIMA.values\n                y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n\n                if (y_pred_ARIMA < 0).any():\n                    rmsle = 9999999\n                else:\n                    rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n\n                mae, mse, rmse, rmsle, r2 = calculate_metrics(test_ARIMA, y_pred_ARIMA)\n\n                print(f'MAE: {mae:.2f}')\n                print(f'MSE: {mse:.2f}')\n                print(f'RMSE: {rmse:.2f}')\n                print(f'RMSLE: {rmsle:.2f}')\n                print(f'R2 score: {r2:.2f}')\n\n                test_['y_pred'] = y_pred_ARIMA.values\n                dataTrain_out = dataTrain_out.append(test_)\n                metrics_df = metrics_df.append({'Model': 'SARIMAX', 'family_encoded': family, 'cols': list(train_.columns), \n                                                'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'R2': r2}, ignore_index=True)\n\n                print('rolling mean RMSLE', metrics_df['RMSLE'].mean())\n                del y_pred_ARIMA, test_exog, series, exogenous\n                gc.collect()\n\n# Use multiprocessing to parallelize the family-wise processing\npool = mp.Pool()\n#dataTrain = dataTrain[dataTrain['family_encoded'] == 12 ]\ndataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\n\npool.map(process_family, dataTrain)\npool.close()\npool.join()\n\ndataTrain_out\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T12:35:58.115883Z","iopub.execute_input":"2023-05-19T12:35:58.116243Z","iopub.status.idle":"2023-05-19T12:53:44.010970Z","shell.execute_reply.started":"2023-05-19T12:35:58.116220Z","shell.execute_reply":"2023-05-19T12:53:44.006283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out","metadata":{"execution":{"iopub.status.busy":"2023-05-19T12:33:35.906964Z","iopub.status.idle":"2023-05-19T12:33:35.908248Z","shell.execute_reply.started":"2023-05-19T12:33:35.907864Z","shell.execute_reply":"2023-05-19T12:33:35.907924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain = dataTrain.sort_values(['family_encoded','date', 'type_encoded'])\n\ndataTrain_out = pd.DataFrame(columns = cols_out)\nmetrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2'])\nglobal y_pred_ARIMA\n\nfor family in tqdm(dataTrain['family_encoded'].unique()):\n    #y_pred_ARIMA\n    print(family)\n    dataTrainFamily =  dataTrain[dataTrain['family_encoded'] == family ]\n\n    #dataTrainFamily = dataTrainFamily.iloc[int(84875):]#[29229395:-1]\n        \n    # Find the last month in the 'date' column\n    last_month = dataTrainFamily['date'].max().to_period('M')\n\n    # Calculate the start date for the test data\n    start_date = last_month.start_time + pd.DateOffset(days=1)\n\n    # Filter the DataFrame based on the dynamic timeframe\n    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n\n    series = train_.copy()\n    #series = series.sort_values(['date','family_encoded'])\n    series.set_index('date', inplace=True)\n    series\n    series.index = series.index.to_period('M')\n\n\n    test_inner = test_.copy()\n    #test_inner = test_inner.sort_values(['date','family_encoded'])\n    test_inner.set_index('date', inplace=True)\n    test_inner\n    test_inner.index = test_inner.index.to_period('M')\n\n    test_exog = test_inner.copy()\n    test_exog = test_exog.drop(['test/train',  'sales'] , axis=1)\n    exogenous = series.drop(['test/train',  'sales'] , axis=1)\n\n    test_ARIMA = test_inner['sales']\n\n\n\n    def fit_sarimax(data,exogenous):\n        # Fit SARIMAX model to the subset of data\n        so = (1, 0, 1, 12)\n        model = SARIMAX(data, order=(1, 0, 4), exog=exogenous, seasonal_order=(so))\n        model_fit = model.fit()\n        return model_fit\n\n    pdqm = [\n    (1, 0, 1, 12)\n    ]\n\n    for so in pdqm:\n        #for i, (train_index, test_index) in enumerate(tscv.split(train_.copy())):\n        #print(f\"pdqm {so}:\")\n\n        exogenous['dcoilwtico_interpolated'] = exogenous['dcoilwtico_interpolated'].astype('int8')\n        # Convert categorical columns to categorical data type\n        exogenous['family_encoded'] = pd.Categorical(exogenous['family_encoded'])\n        exogenous['type_encoded'] = pd.Categorical(exogenous['type_encoded'])\n        exogenous['month'] = pd.Categorical(exogenous['month'])\n        exogenous['day_of_month'] = pd.Categorical(exogenous['day_of_month'])\n        exogenous['week_of_year'] = pd.Categorical(exogenous['week_of_year'])\n        exogenous['day_of_week'] = pd.Categorical(exogenous['day_of_week'])\n\n        series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n        test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n\n        test_exog['dcoilwtico_interpolated'] = test_exog['dcoilwtico_interpolated'].astype('int8')\n        # Convert categorical columns to categorical data type\n        test_exog['family_encoded'] = pd.Categorical(test_exog['family_encoded'])\n        test_exog['type_encoded'] = pd.Categorical(test_exog['type_encoded'])\n        test_exog['month'] = pd.Categorical(test_exog['month'])\n        test_exog['day_of_month'] = pd.Categorical(test_exog['day_of_month'])\n        test_exog['week_of_year'] = pd.Categorical(test_exog['week_of_year'])\n        test_exog['day_of_week'] = pd.Categorical(test_exog['day_of_week'])        \n    \n\n\n        # Get the memory usage of each column\n        memory_usage = exogenous.memory_usage(deep=True)\n        #print(\"\\nMemory Usage:\")\n        #print(memory_usage)\n        \n        # Calculate memory usage in bytes\n        memory_bytes = exogenous.memory_usage(deep=True).sum()\n        # Convert to gigabytes (GB)\n        memory_gb = memory_bytes / (1024**3)\n        # Print the size in GB\n        print(f\"Size of data: {memory_gb:.2f} GB\")\n\n        start_time = time.time()\n        #model = ARIMA(series, order=(1,0,4), exog=exogenous )\n\n\n        model = SARIMAX(series['sales'], order=(1, 0, 4), exog=exogenous, seasonal_order=(so))\n        model_fit = model.fit(disp=False)\n        del model\n        \n        # Calculate the fitting time\n        fitting_time = time.time() - start_time\n\n        # Print the fitting time\n        print(\"Fitting time:\", fitting_time, \"seconds\")\n\n\n        y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog= test_exog)\n\n        test_['y_pred_raw'] = y_pred_ARIMA.values\n\n        y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n\n\n        if (y_pred_ARIMA < 0).any():\n            rmsle = 9999999\n\n        else:    \n            #print((test_ARIMA.shape),(y_pred_ARIMA.shape))\n            #print(type(test_ARIMA),type(y_pred_ARIMA.shape))\n            rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n\n        # Evaluate the model\n        mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n        mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n        rmse = np.sqrt(mse)\n\n        r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n\n        print(f'MAE: {mae:.2f}')\n        print(f'MSE: {mse:.2f}')\n        print(f'RMSE: {rmse:.2f}')\n        print(f'RMSLE: {rmsle:.2f}')\n        print(f'R2 score: {r2:.2f}')\n\n\n        test_['y_pred'] = y_pred_ARIMA.values\n        \n        dataTrain_out = dataTrain_out.append(test_)\n\n        metrics_df = metrics_df.append({'Model': 'SARIMAX', 'family_encoded':family , 'cols': list(train_.columns),  'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'RMSLE': rmsle, 'R2': r2}, ignore_index=True)\n        print('rolling mean RMSLE',metrics_df['RMSLE'].mean())\n        del y_pred_ARIMA, test_exog, series, exogenous\n        gc.collect()\n        \n        #print(dataTrainFamily)\ndataTrain_out\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T10:15:45.309322Z","iopub.execute_input":"2023-05-19T10:15:45.309747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean RMSLE',metrics_df['RMSLE'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df = metrics_df.sort_values(by='RMSLE')\nmetrics_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_df.to_csv('metrics_df.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain_out.to_csv('dataTrain_out.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#!pip install nbeats-keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom nbeats_keras.model import NBeatsNet\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:12:40.502376Z","iopub.execute_input":"2023-05-20T15:12:40.502848Z","iopub.status.idle":"2023-05-20T15:12:40.514286Z","shell.execute_reply.started":"2023-05-20T15:12:40.502796Z","shell.execute_reply":"2023-05-20T15:12:40.513292Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\n# Define custom RMSLE metric\ndef rmsle(y_true, y_pred):\n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = K.square(y_true_log - y_pred_log)\n    mean_squared_diff = K.mean(squared_diff, axis=-1)\n    rmsle = K.sqrt(mean_squared_diff)\n    return rmsle\n\n\n# Find the last month in the 'date' column\nlast_month = dataTrain['date'].max().to_period('M')\n\n# Calculate the start date for the test data\nstart_date = last_month.start_time + pd.DateOffset(days=1)\n\n# Filter the DataFrame based on the dynamic timeframe\ntrain_ = dataTrain[dataTrain['date'] < start_date]\ntest_ = dataTrain[dataTrain['date'] >= start_date]\n\ntrain_.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-20T09:59:34.273735Z","iopub.execute_input":"2023-05-20T09:59:34.274228Z","iopub.status.idle":"2023-05-20T09:59:42.815088Z","shell.execute_reply.started":"2023-05-20T09:59:34.274189Z","shell.execute_reply":"2023-05-20T09:59:42.813674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_","metadata":{"execution":{"iopub.status.busy":"2023-05-20T10:04:52.434129Z","iopub.execute_input":"2023-05-20T10:04:52.434783Z","iopub.status.idle":"2023-05-20T10:04:52.474748Z","shell.execute_reply.started":"2023-05-20T10:04:52.434734Z","shell.execute_reply":"2023-05-20T10:04:52.473852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom nbeats_keras.model import NBeatsNet\nimport tensorflow as tf\n\n\n# Assuming your data is stored in variables X and y\nX = train_.drop(['sales', 'test/train', 'date'], axis=1).values\ny = train_['sales'].values\n\n# Normalize the input features\nX_mean = np.mean(X, axis=0)\nX_std = np.std(X, axis=0)\nX_normalized = (X - X_mean) / X_std\nprint(X_normalized.shape)\n\n# Define the NBeatsNet model\ninput_shape = X_normalized.shape[1]\noutput_shape = 1  # Assuming you have a single output target\nbackcast_length = 10  # Length of the backcast sequence\nforecast_length = 1  # Length of the forecast sequence\nstack_layers = 30  # Number of stack layers\nstack_width = 512  # Width of each stack\noutput_activation = 'linear'  # Activation function for output layer\n\n# Define the model architecture\nmodel = NBeatsNet(input_dim=1, output_dim=1, stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n                  nb_blocks_per_stack=3, forecast_length=forecast_length, backcast_length=backcast_length,\n                  thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=64)\n\n# Compile the model\nmodel.compile(optimizer=Adam(), loss='mse')\n\nX_normalized = #tf.convert_to_tensor(?????), dtype=tf.float32)\n\n# Train the model\nearly_stopping = EarlyStopping(patience=5, restore_best_weights=True)\nmodel.fit(X_normalized, y, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T10:29:12.001522Z","iopub.execute_input":"2023-05-20T10:29:12.002010Z","iopub.status.idle":"2023-05-20T10:29:12.602465Z","shell.execute_reply.started":"2023-05-20T10:29:12.001975Z","shell.execute_reply":"2023-05-20T10:29:12.600809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom nbeats_keras.model import NBeatsNet\nimport tensorflow as tf\nfrom keras import backend as K\n\n\ndataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\n#dataTrain = dataTrain[dataTrain['family_encoded'].isin([12])]\n\ntest_appened = pd.DataFrame(columns = list(dataTrain.columns))\n\n# Compile the model\ndef rmsle_loss(y_true, y_pred):\n\n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = K.square(y_true_log - y_pred_log)\n    mean_squared_diff = K.mean(squared_diff, axis=-1)\n    rmsle = K.sqrt(mean_squared_diff)\n    return rmsle\n\n\nmodels = []\n\nfor family in tqdm(dataTrain['family_encoded'].unique()):\n    print(family)\n    dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family]\n        \n    # Find the last month in the 'date' column\n    last_month = dataTrainFamily['date'].max().to_period('M')\n\n    # Calculate the start date for the test data\n    start_date = last_month.start_time + pd.DateOffset(days=1)\n\n    # Filter the DataFrame based on the dynamic timeframe\n    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n    \n    test_appened = test_appened.append(test_)\n\n    # Assuming your data is stored in variables X and y\n    X = train_.drop(['sales', 'test/train', 'date'], axis=1).values\n    y = train_['sales'].values\n    \n    # Define the model architecture\n    model = NBeatsNet(input_dim=dataTrain.drop(['sales', 'test/train', 'date'], axis=1).shape[1], output_dim=1, stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n                      nb_blocks_per_stack=3, forecast_length=1, backcast_length=1,\n                      thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=64)\n\n    # Compile the model with RMSLE loss\n    model.compile(loss=rmsle_loss, optimizer='adam')#model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmsle])\n\n\n    # Normalize the input features\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    X_normalized = (X - X_mean) / X_std\n\n    # Define the NBeatsNet model\n    input_shape = X_normalized.shape[1]\n    print(input_shape)\n    output_shape = 1  # Assuming you have a single output target\n    backcast_length =  train_['family_encoded'].nunique()  # Length of the backcast sequence\n    forecast_length = 1  # Length of the forecast sequence\n    stack_layers = 30  # Number of stack layers\n    stack_width = 512  # Width of each stack\n    output_activation = 'linear'  # Activation function for output layer\n\n\n\n\n    # Reshape the input data\n    num_samples = X_normalized.shape[0]\n    print(X_normalized.shape)\n    X_reshaped = np.reshape(X_normalized, ( int(num_samples / train_['family_encoded'].nunique()), train_['family_encoded'].nunique(), input_shape))\n    X_tensor = tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n\n    # Train the model\n    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n    model.fit(X_tensor, y, epochs=3, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n    models.append(model)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:08:15.748881Z","iopub.execute_input":"2023-05-20T12:08:15.749271Z","iopub.status.idle":"2023-05-20T12:11:39.841917Z","shell.execute_reply.started":"2023-05-20T12:08:15.749238Z","shell.execute_reply":"2023-05-20T12:11:39.840933Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"12\n8\n(11502, 8)\nEpoch 1/3\n288/288 [==============================] - 28s 35ms/step - loss: 1.5084 - val_loss: 1.0200\nEpoch 2/3\n288/288 [==============================] - 9s 30ms/step - loss: 0.7368 - val_loss: 0.9465\nEpoch 3/3\n288/288 [==============================] - 8s 29ms/step - loss: 0.7313 - val_loss: 0.9644\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 1/4 [00:47<02:22, 47.58s/it]","output_type":"stream"},{"name":"stdout","text":"13\n8\n(11502, 8)\nEpoch 1/3\n288/288 [==============================] - 28s 34ms/step - loss: 1.2715 - val_loss: 1.0609\nEpoch 2/3\n288/288 [==============================] - 8s 28ms/step - loss: 1.1867 - val_loss: 1.1148\nEpoch 3/3\n288/288 [==============================] - 8s 29ms/step - loss: 1.1862 - val_loss: 1.1593\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 2/4 [01:34<01:34, 47.18s/it]","output_type":"stream"},{"name":"stdout","text":"14\n8\n(11502, 8)\nEpoch 1/3\n288/288 [==============================] - 27s 34ms/step - loss: 0.6277 - val_loss: 0.6540\nEpoch 2/3\n288/288 [==============================] - 8s 29ms/step - loss: 0.6235 - val_loss: 0.6695\nEpoch 3/3\n288/288 [==============================] - 8s 28ms/step - loss: 0.6241 - val_loss: 0.6640\n","output_type":"stream"},{"name":"stderr","text":" 75%|███████▌  | 3/4 [02:28<00:50, 50.49s/it]","output_type":"stream"},{"name":"stdout","text":"15\n8\n(11502, 8)\nEpoch 1/3\n288/288 [==============================] - 28s 34ms/step - loss: 0.9323 - val_loss: 0.8016\nEpoch 2/3\n288/288 [==============================] - 8s 27ms/step - loss: 0.7875 - val_loss: 0.8248\nEpoch 3/3\n288/288 [==============================] - 8s 29ms/step - loss: 0.7856 - val_loss: 0.8767\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4/4 [03:24<00:00, 51.02s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_all = [] \ny_pred_all = []\nfor model, family in zip(models, test_appened['family_encoded'].unique()):\n    # Assuming your test data is stored in variables X_test and y_test\n    test = test_appened[test_appened['family_encoded'] == family]\n\n    X_test = test.drop(['sales', 'test/train', 'date'], axis=1).values\n    y_test = test['sales'].values\n\n    # Normalize the input features using the same mean and standard deviation from training data\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    X_std_nonzero = X_std.copy()\n    X_std_nonzero[X_std_nonzero == 0] = 1  # Replace zero standard deviations with 1 to avoid division by zero\n    X_test_normalized = (X_test - X_mean) / X_std_nonzero\n\n\n    # Reshape the input data\n    num_samples_test = X_test_normalized.shape[0]\n    X_test_reshaped = np.reshape(X_test_normalized, (int(num_samples_test / backcast_length), backcast_length, input_shape))\n    X_tensor = tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n\n    # Predict on the test data\n    y_pred = model.predict(X_tensor)\n\n    # Perform any necessary post-processing on the predicted values\n\n    # Evaluate the model's performance on the test data\n    loss = rmsle_loss(y_test, y_pred)\n    print(f\"Test Loss: {np.mean(loss)}\")\n    #print(f\"Test Loss: {loss}\")\n    y_test_all.append(y_test)\n    y_pred_all.append(y_pred)\n    #y_test_all = np.append(y_test_all, y_test, axis=0)\n    #y_pred_all = np.append(y_pred_all, y_pred, axis=0)\n\n\n\nloss = rmsle_loss(list(y_test_all), list(y_pred_all))\ny_teslosst_all = np.reshape(loss, (-1, loss.shape[-1]))\nprint(f\" Total Test Loss: {np.mean(y_teslosst_all)}\")\nprint(f\"Test Loss: {loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:36:14.053871Z","iopub.execute_input":"2023-05-20T12:36:14.054241Z","iopub.status.idle":"2023-05-20T12:36:31.296387Z","shell.execute_reply.started":"2023-05-20T12:36:14.054210Z","shell.execute_reply":"2023-05-20T12:36:31.290226Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"360/360 [==============================] - 3s 9ms/step\nTest Loss: 0.5242944359779358\n360/360 [==============================] - 3s 9ms/step\nTest Loss: 1.1064798831939697\n360/360 [==============================] - 3s 9ms/step\nTest Loss: 0.6291805505752563\n360/360 [==============================] - 3s 9ms/step\nTest Loss: 0.7509424090385437\n Total Test Loss: 3.210923671722412\nTest Loss: [[[0.53891844 5.2966223  7.474173   4.950731  ]\n  [0.53891844 5.2966223  7.474173   4.950731  ]\n  [0.53891844 5.2966223  7.474173   4.950731  ]\n  ...\n  [0.51569295 5.456182   7.636218   5.111582  ]\n  [0.51569295 5.456182   7.636218   5.111582  ]\n  [0.51569295 5.456182   7.636218   5.111582  ]]\n\n [[5.7647886  1.1074593  1.9658301  1.0098543 ]\n  [5.7647886  1.1074593  1.9658301  1.0098543 ]\n  [5.7647886  1.1074593  1.9658301  1.0098543 ]\n  ...\n  [5.7655716  1.107737   1.9650855  1.0103912 ]\n  [5.7655716  1.107737   1.9650855  1.0103912 ]\n  [5.7655716  1.107737   1.9650855  1.0103912 ]]\n\n [[7.647302   2.5035212  0.6290671  2.681309  ]\n  [7.647302   2.5035212  0.6290671  2.681309  ]\n  [7.647302   2.5035212  0.6290671  2.681309  ]\n  ...\n  [7.6541796  2.5097973  0.62938505 2.687936  ]\n  [7.6541796  2.5097973  0.62938505 2.687936  ]\n  [7.6541796  2.5097973  0.62938505 2.687936  ]]\n\n [[5.2302446  1.0464722  2.48051    0.75336915]\n  [5.2302446  1.0464722  2.48051    0.75336915]\n  [5.2302446  1.0464722  2.48051    0.75336915]\n  ...\n  [5.2087603  1.0497048  2.5014014  0.7492917 ]\n  [5.2087603  1.0497048  2.5014014  0.7492917 ]\n  [5.2087603  1.0497048  2.5014014  0.7492917 ]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\" Total Test Loss: {np.mean(y_teslosst_all)}\")\nprint(f\"Test Loss: {y_teslosst_all}\")\ny_teslosst_all","metadata":{"execution":{"iopub.status.busy":"2023-05-20T14:40:01.236039Z","iopub.execute_input":"2023-05-20T14:40:01.236394Z","iopub.status.idle":"2023-05-20T14:40:01.598197Z","shell.execute_reply.started":"2023-05-20T14:40:01.236363Z","shell.execute_reply":"2023-05-20T14:40:01.595562Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Total Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(y_teslosst_all)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_teslosst_all\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m y_teslosst_all\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import numpy as np\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom nbeats_keras.model import NBeatsNet\nimport tensorflow as tf\nfrom keras import backend as K\n\n\ndataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\n#dataTrain = dataTrain[dataTrain['family_encoded'].isin([12])]\n\ntest_appened = pd.DataFrame(columns = list(dataTrain.columns))\n\n# Define the model architecture\nmodel = NBeatsNet(input_dim=dataTrain.drop(['sales', 'test/train', 'date'], axis=1).shape[1], output_dim=1, stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n                  nb_blocks_per_stack=3, forecast_length=1, backcast_length=1,\n                  thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=64)\n\n# Compile the model\ndef rmsle_loss(y_true, y_pred):\n\n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = K.square(y_true_log - y_pred_log)\n    mean_squared_diff = K.mean(squared_diff, axis=-1)\n    rmsle = K.sqrt(mean_squared_diff)\n    return rmsle\n\n# Compile the model with RMSLE loss\nmodel.compile(loss=rmsle_loss, optimizer='adam')#model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmsle])\n\n\nfor family in tqdm(dataTrain['family_encoded'].unique()):\n    print(family)\n    dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family]\n        \n    # Find the last month in the 'date' column\n    last_month = dataTrainFamily['date'].max().to_period('M')\n\n    # Calculate the start date for the test data\n    start_date = last_month.start_time + pd.DateOffset(days=1)\n\n    # Filter the DataFrame based on the dynamic timeframe\n    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date]\n    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date]\n    \n    test_appened = test_appened.append(test_)\n\n    # Assuming your data is stored in variables X and y\n    X = train_.drop(['sales', 'test/train', 'date'], axis=1).values\n    y = train_['sales'].values\n\n\n    # Normalize the input features\n    X_mean = np.mean(X, axis=0)\n    X_std = np.std(X, axis=0)\n    X_normalized = (X - X_mean) / X_std\n\n    # Define the NBeatsNet model\n    input_shape = X_normalized.shape[1]\n    print(input_shape)\n    output_shape = 1  # Assuming you have a single output target\n    backcast_length =  train_['family_encoded'].nunique()  # Length of the backcast sequence\n    forecast_length = 1  # Length of the forecast sequence\n    stack_layers = 30  # Number of stack layers\n    stack_width = 512  # Width of each stack\n    output_activation = 'linear'  # Activation function for output layer\n\n\n\n\n    # Reshape the input data\n    num_samples = X_normalized.shape[0]\n    print(X_normalized.shape)\n    X_reshaped = np.reshape(X_normalized, ( int(num_samples / train_['family_encoded'].nunique()), train_['family_encoded'].nunique(), input_shape))\n    X_tensor = tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n\n    # Train the model\n    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n    model.fit(X_tensor, y, epochs=3, batch_size=32, validation_split=0.2, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:37:36.725712Z","iopub.execute_input":"2023-05-20T12:37:36.726380Z","iopub.status.idle":"2023-05-20T12:37:43.981960Z","shell.execute_reply.started":"2023-05-20T12:37:36.726339Z","shell.execute_reply":"2023-05-20T12:37:43.979770Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"12\n8\n(11502, 8)\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/4 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nbeats_keras/model.py:233\u001b[0m, in \u001b[0;36mNBeatsNet.__getattr__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m keract\u001b[38;5;241m.\u001b[39mget_activations(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, x\u001b[38;5;241m=\u001b[39margs)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intermediary_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    231\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m: a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: b} \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(a)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstack_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m     ]\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:928\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 928\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    931\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    932\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:749\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_internal_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    754\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:162\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 162\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;66;03m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m generalized_func_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_capture_func_lib  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Maintain the list of all captures\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m monomorphic_function\u001b[38;5;241m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:645\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    642\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    643\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 645\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1258\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1258\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n","File \u001b[0;32m/tmp/__autograph_generated_fileo19ghqik.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1230\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[1;32m   1232\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m     outputs,\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1237\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1238\u001b[0m )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2895\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3695\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 3696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1222\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1023\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# Run forward pass.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m-> 1023\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py:511\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/functional.py:668\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    672\u001b[0m     node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)\n\u001b[1;32m    673\u001b[0m ):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/layers/core/dense.py:255\u001b[0m, in \u001b[0;36mDense.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mbias_add(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_ragged:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m original_inputs\u001b[38;5;241m.\u001b[39mwith_flat_values(outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/activations.py:317\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.activations.relu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrelu\u001b[39m(x, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m    282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the rectified linear unit activation function.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03m    With default values, this returns the standard ReLU activation:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m        Tensor will be of the same shape and dtype of input `x`.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/backend.py:5369\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[1;32m   5367\u001b[0m     clip_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 5369\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_max:\n\u001b[1;32m   5372\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m _constant_to_tensor(max_value, x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:10741\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(features, name)\u001b[0m\n\u001b[1;32m  10739\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m  10740\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m> 10741\u001b[0m   _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10742\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10743\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m  10744\u001b[0m   _result \u001b[38;5;241m=\u001b[39m _dispatch\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m  10745\u001b[0m         relu, (), \u001b[38;5;28mdict\u001b[39m(features\u001b[38;5;241m=\u001b[39mfeatures, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m  10746\u001b[0m       )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    790\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[1;32m    791\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[1;32m    793\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[1;32m    794\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:749\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    747\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[1;32m    748\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFuncGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3807\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3797\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m   3798\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation(\n\u001b[1;32m   3799\u001b[0m       node_def,\n\u001b[1;32m   3800\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3805\u001b[0m       original_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_original_op,\n\u001b[1;32m   3806\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m-> 3807\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3879\u001b[0m, in \u001b[0;36mGraph._create_op_helper\u001b[0;34m(self, op, compute_device)\u001b[0m\n\u001b[1;32m   3876\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   3877\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 3879\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_op_seen_by_control_dependencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_device:\n\u001b[1;32m   3882\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_device_functions(op)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"test_appened","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:01:42.425409Z","iopub.execute_input":"2023-05-20T12:01:42.425728Z","iopub.status.idle":"2023-05-20T12:01:42.450770Z","shell.execute_reply.started":"2023-05-20T12:01:42.425695Z","shell.execute_reply":"2023-05-20T12:01:42.449765Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       test/train   sales store_nbr       date month day_of_month  \\\n7041        train 3247.00         1 2017-08-02     8            2   \n7074        train 2177.00         1 2017-08-03     8            3   \n7107        train 2778.00         1 2017-08-04     8            4   \n7140        train 2034.00         1 2017-08-05     8            5   \n7173        train 1025.00         1 2017-08-06     8            6   \n...           ...     ...       ...        ...   ...          ...   \n404364      train   75.00         9 2017-08-11     8           11   \n404397      train   57.00         9 2017-08-12     8           12   \n404430      train   58.00         9 2017-08-13     8           13   \n404463      train   41.00         9 2017-08-14     8           14   \n404496      train   64.00         9 2017-08-15     8           15   \n\n       week_of_year day_of_week  dcoilwtico_interpolated family_encoded  \\\n7041             31           3                    49.27             12   \n7074             31           4                    49.40             12   \n7107             31           5                    49.37             12   \n7140             31           6                    49.50             12   \n7173             31           7                    49.44             12   \n...             ...         ...                      ...            ...   \n404364           32           5                    48.58             15   \n404397           32           6                    48.40             15   \n404430           32           7                    48.00             15   \n404463           33           1                    47.72             15   \n404496           33           2                    47.32             15   \n\n       type_encoded  \n7041              3  \n7074              3  \n7107              3  \n7140              3  \n7173              3  \n...             ...  \n404364            1  \n404397            1  \n404430            1  \n404463            1  \n404496            1  \n\n[3024 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test/train</th>\n      <th>sales</th>\n      <th>store_nbr</th>\n      <th>date</th>\n      <th>month</th>\n      <th>day_of_month</th>\n      <th>week_of_year</th>\n      <th>day_of_week</th>\n      <th>dcoilwtico_interpolated</th>\n      <th>family_encoded</th>\n      <th>type_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7041</th>\n      <td>train</td>\n      <td>3247.00</td>\n      <td>1</td>\n      <td>2017-08-02</td>\n      <td>8</td>\n      <td>2</td>\n      <td>31</td>\n      <td>3</td>\n      <td>49.27</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7074</th>\n      <td>train</td>\n      <td>2177.00</td>\n      <td>1</td>\n      <td>2017-08-03</td>\n      <td>8</td>\n      <td>3</td>\n      <td>31</td>\n      <td>4</td>\n      <td>49.40</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7107</th>\n      <td>train</td>\n      <td>2778.00</td>\n      <td>1</td>\n      <td>2017-08-04</td>\n      <td>8</td>\n      <td>4</td>\n      <td>31</td>\n      <td>5</td>\n      <td>49.37</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7140</th>\n      <td>train</td>\n      <td>2034.00</td>\n      <td>1</td>\n      <td>2017-08-05</td>\n      <td>8</td>\n      <td>5</td>\n      <td>31</td>\n      <td>6</td>\n      <td>49.50</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7173</th>\n      <td>train</td>\n      <td>1025.00</td>\n      <td>1</td>\n      <td>2017-08-06</td>\n      <td>8</td>\n      <td>6</td>\n      <td>31</td>\n      <td>7</td>\n      <td>49.44</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>404364</th>\n      <td>train</td>\n      <td>75.00</td>\n      <td>9</td>\n      <td>2017-08-11</td>\n      <td>8</td>\n      <td>11</td>\n      <td>32</td>\n      <td>5</td>\n      <td>48.58</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>404397</th>\n      <td>train</td>\n      <td>57.00</td>\n      <td>9</td>\n      <td>2017-08-12</td>\n      <td>8</td>\n      <td>12</td>\n      <td>32</td>\n      <td>6</td>\n      <td>48.40</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>404430</th>\n      <td>train</td>\n      <td>58.00</td>\n      <td>9</td>\n      <td>2017-08-13</td>\n      <td>8</td>\n      <td>13</td>\n      <td>32</td>\n      <td>7</td>\n      <td>48.00</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>404463</th>\n      <td>train</td>\n      <td>41.00</td>\n      <td>9</td>\n      <td>2017-08-14</td>\n      <td>8</td>\n      <td>14</td>\n      <td>33</td>\n      <td>1</td>\n      <td>47.72</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>404496</th>\n      <td>train</td>\n      <td>64.00</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3024 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n\n\n\n# Assuming your test data is stored in variables X_test and y_test\nX_test = test_appened.drop(['sales', 'test/train', 'date'], axis=1).values\ny_test = test_appened['sales'].values\n\n# Normalize the input features using the same mean and standard deviation from training data\nX_mean = np.mean(X, axis=0)\nX_std = np.std(X, axis=0)\nX_std_nonzero = X_std.copy()\nX_std_nonzero[X_std_nonzero == 0] = 1  # Replace zero standard deviations with 1 to avoid division by zero\nX_test_normalized = (X_test - X_mean) / X_std_nonzero\n\n\n# Reshape the input data\nnum_samples_test = X_test_normalized.shape[0]\nX_test_reshaped = np.reshape(X_test_normalized, (int(num_samples_test / backcast_length), backcast_length, input_shape))\nX_tensor = tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n\n# Predict on the test data\ny_pred = model.predict(X_tensor)\n\n# Perform any necessary post-processing on the predicted values\n\n# Evaluate the model's performance on the test data\nloss = rmsle_loss(y_test, y_pred)\nprint(f\"Test Loss: {np.mean(loss)}\")\nprint(f\"Test Loss: {loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:01:42.452433Z","iopub.execute_input":"2023-05-20T12:01:42.452856Z","iopub.status.idle":"2023-05-20T12:01:49.460690Z","shell.execute_reply.started":"2023-05-20T12:01:42.452822Z","shell.execute_reply":"2023-05-20T12:01:49.459575Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"360/360 [==============================] - 5s 9ms/step\nTest Loss: 2.9450061321258545\nTest Loss: [[2.948526 ]\n [2.948526 ]\n [2.948526 ]\n ...\n [2.9431324]\n [2.9431324]\n [2.9431324]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom nbeats_keras.model import NBeatsNet\nimport tensorflow as tf\n\n#dataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\ndataTrain = dataTrain[dataTrain['family_encoded'].isin([12])]\n\n# Find the last month in the 'date' column\nlast_month = dataTrain['date'].max().to_period('M')\n\n# Calculate the start date for the test data\nstart_date = last_month.start_time + pd.DateOffset(days=1)\n\n# Filter the DataFrame based on the dynamic timeframe\ntrain_ = dataTrain[dataTrain['date'] < start_date]\ntest_ = dataTrain[dataTrain['date'] >= start_date]\n\n\n# Assuming your data is stored in variables X and y\nX = train_.drop(['sales', 'test/train', 'date'], axis=1).values\ny = train_['sales'].values\n\n\n# Normalize the input features\nX_mean = np.mean(X, axis=0)\nX_std = np.std(X, axis=0)\nX_normalized = (X - X_mean) / X_std\n\n# Define the NBeatsNet model\ninput_shape = X_normalized.shape[1]\noutput_shape = 1  # Assuming you have a single output target\nbackcast_length =  train_['family_encoded'].nunique()  # Length of the backcast sequence\nforecast_length = 1  # Length of the forecast sequence\nstack_layers = 30  # Number of stack layers\nstack_width = 512  # Width of each stack\noutput_activation = 'linear'  # Activation function for output layer\n\n# Define the model architecture\nmodel = NBeatsNet(input_dim=8, output_dim=1, stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n                  nb_blocks_per_stack=3, forecast_length=forecast_length, backcast_length=backcast_length,\n                  thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=64)\n\n# Compile the model\ndef rmsle_loss(y_true, y_pred):\n    \n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = K.square(y_true_log - y_pred_log)\n    mean_squared_diff = K.mean(squared_diff, axis=-1)\n    rmsle = K.sqrt(mean_squared_diff)\n    return rmsle\n\n# Compile the model with RMSLE loss\nmodel.compile(loss=rmsle_loss, optimizer='adam')#model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmsle])\n\n\n# Reshape the input data\nnum_samples = X_normalized.shape[0]\nprint(X_normalized.shape)\nX_reshaped = np.reshape(X_normalized, ( int(num_samples / train_['family_encoded'].nunique()), train_['family_encoded'].nunique(), input_shape))\nX_tensor = tf.convert_to_tensor(X_reshaped, dtype=tf.float32)\n\n# Train the model\nearly_stopping = EarlyStopping(patience=5, restore_best_weights=True)\nmodel.fit(X_tensor, y, epochs=10, batch_size=16, validation_split=0.2, callbacks=[early_stopping])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain = dataTrain[dataTrain['family_encoded'].isin([12, 13, 14, 15])]\n#dataTrain = dataTrain[dataTrain['family_encoded'].isin([12])]\n\n# Find the last month in the 'date' column\nlast_month = dataTrain['date'].max().to_period('M')\n\n# Calculate the start date for the test data\nstart_date = last_month.start_time + pd.DateOffset(days=1)\n\n# Filter the DataFrame based on the dynamic timeframe\ntrain_ = dataTrain[dataTrain['date'] < start_date]\ntest_ = dataTrain[dataTrain['date'] >= start_date]\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:15:26.862953Z","iopub.execute_input":"2023-05-20T16:15:26.864095Z","iopub.status.idle":"2023-05-20T16:15:26.885975Z","shell.execute_reply.started":"2023-05-20T16:15:26.864063Z","shell.execute_reply":"2023-05-20T16:15:26.885071Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\ntrain_data = train_.drop(['sales', 'test/train', 'date'], axis=1).values\ntrain_targets = train_['sales'].values\ntrain_data.shape\n\n# Define the input shape based on your data dimensions\ninput_shape = (train_data.shape[1], 1)\nprint (input_shape)\nprint (train_data.shape)\n\n# Create the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=input_shape))  # 64 is the number of LSTM units (you can adjust this value)\nmodel.add(Dense(1))  # Output layer with a single neuron for regression (adjust as needed)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')  # Adjust optimizer and loss function as per your task\n\n# Train the model\nmodel.fit(train_data, train_targets, epochs=10, batch_size=256)\n\n# Once trained, you can use the model to make predictions on new data\ntest_data = np.random.random((10, 8))  # Replace this with your actual test data\ntest_data = (test_.shape[1], 1)\n#predictions = model.predict(test_data)\n\n# Print the predicted values\n#print(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T15:39:30.992529Z","iopub.execute_input":"2023-05-20T15:39:30.993147Z","iopub.status.idle":"2023-05-20T15:39:35.982753Z","shell.execute_reply.started":"2023-05-20T15:39:30.993102Z","shell.execute_reply":"2023-05-20T15:39:35.981884Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(8, 1)\n(11502, 8)\nEpoch 1/10\n45/45 [==============================] - 3s 4ms/step - loss: 31079334.0000\nEpoch 2/10\n45/45 [==============================] - 0s 4ms/step - loss: 31000152.0000\nEpoch 3/10\n45/45 [==============================] - 0s 4ms/step - loss: 30948670.0000\nEpoch 4/10\n45/45 [==============================] - 0s 4ms/step - loss: 30913628.0000\nEpoch 5/10\n45/45 [==============================] - 0s 4ms/step - loss: 30881818.0000\nEpoch 6/10\n45/45 [==============================] - 0s 4ms/step - loss: 30851282.0000\nEpoch 7/10\n45/45 [==============================] - 0s 6ms/step - loss: 30821594.0000\nEpoch 8/10\n45/45 [==============================] - 0s 4ms/step - loss: 30792430.0000\nEpoch 9/10\n45/45 [==============================] - 0s 4ms/step - loss: 30763614.0000\nEpoch 10/10\n45/45 [==============================] - 0s 4ms/step - loss: 30735070.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\n\n# Assuming your training data is stored in a variable named 'data'\ntrain_data = train_.drop(['sales', 'test/train', 'date'], axis=1).values\ntrain_targets = train_['sales'].values # Replace this with your actual training targets\n\nprint (input_shape)\nprint (train_data.shape)\n\n# Reshape the input data to have a 3D shape\n#train_data_reshaped = pd.wide_to_long(train_data, stubnames='', i='index', j='timestep', sep='').reset_index()\n#train_data_reshaped = pd.pivot(train_data_reshaped, index='player', columns='team', values='points')\n\ntrain_data = train_data.reshape((int(train_data.shape[0]/ train_['family_encoded'].nunique()), train_data.shape[1],  train_['family_encoded'].nunique()))\nprint(train_data.shape)\nprint((train_data.shape[1], train_data.shape[2]))\n\n# Create the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(16, input_shape=(train_data.shape[1], train_data.shape[2])))\nmodel.add(Dense(1))\n\n# Compile and train the model as before\nmodel.compile(optimizer='adam', loss='mse')\nprint(model.summary())\nmodel.fit(train_data, train_targets, epochs=1, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:31:01.457468Z","iopub.execute_input":"2023-05-20T16:31:01.457871Z","iopub.status.idle":"2023-05-20T16:31:01.799248Z","shell.execute_reply.started":"2023-05-20T16:31:01.457839Z","shell.execute_reply":"2023-05-20T16:31:01.797774Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"(8, 1)\n(46008, 8)\n(11502, 8, 4)\n(8, 4)\nModel: \"sequential_16\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_16 (LSTM)              (None, 16)                1344      \n                                                                 \n dense_16 (Dense)            (None, 4)                 68        \n                                                                 \n=================================================================\nTotal params: 1,412\nTrainable params: 1,412\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[43], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m---> 28\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1842\u001b[0m         label,\n\u001b[1;32m   1843\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1844\u001b[0m             \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[1;32m   1845\u001b[0m         ),\n\u001b[1;32m   1846\u001b[0m     )\n\u001b[1;32m   1847\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n","\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 11502\n  y sizes: 46008\nMake sure all arrays contain the same number of samples."],"ename":"ValueError","evalue":"Data cardinality is ambiguous:\n  x sizes: 11502\n  y sizes: 46008\nMake sure all arrays contain the same number of samples.","output_type":"error"}]},{"cell_type":"code","source":"\n# Creating a dictionary\nd = {\n    'A':[1,1,1,1,1,2],\n}\n\n# Creating a dataframe\ndf = pd.DataFrame(d)\ndf['B'] = df['A']\n\n# Display original DataFrame\nprint(\"Original DataFrame:\\n\",df,\"\\n\")\n\n# Creating a new column\ndf['C'] = df['B'].diff()\n\n# Filter result\nres = df[df['C'] != 0].index\n\n# Display result\nprint(\"Result:\\n\",res[1:],\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T16:57:16.806593Z","iopub.execute_input":"2023-05-20T16:57:16.807420Z","iopub.status.idle":"2023-05-20T16:57:16.821196Z","shell.execute_reply.started":"2023-05-20T16:57:16.807385Z","shell.execute_reply":"2023-05-20T16:57:16.820259Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Original DataFrame:\n    A  B\n0  1  1\n1  1  1\n2  1  1\n3  1  1\n4  1  1\n5  2  2 \n\nResult:\n Int64Index([5], dtype='int64') \n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataTrain.sort_values(by=['date','family_encoded']).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-20T17:05:44.649842Z","iopub.execute_input":"2023-05-20T17:05:44.650324Z","iopub.status.idle":"2023-05-20T17:05:44.691055Z","shell.execute_reply.started":"2023-05-20T17:05:44.650283Z","shell.execute_reply":"2023-05-20T17:05:44.690187Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"      test/train  sales  store_nbr       date  month  day_of_month  \\\n0          train   0.00          1 2017-01-01      1             1   \n1          train   0.00         10 2017-01-01      1             1   \n2          train   0.00         11 2017-01-01      1             1   \n3          train   0.00         12 2017-01-01      1             1   \n4          train   0.00         13 2017-01-01      1             1   \n...          ...    ...        ...        ...    ...           ...   \n49027      train  24.00         54 2017-08-15      8            15   \n49028      train  24.00          6 2017-08-15      8            15   \n49029      train  42.00          7 2017-08-15      8            15   \n49030      train  40.00          8 2017-08-15      8            15   \n49031      train  64.00          9 2017-08-15      8            15   \n\n       week_of_year  day_of_week  dcoilwtico_interpolated  family_encoded  \\\n0                52            7                    53.05              12   \n1                52            7                    53.05              12   \n2                52            7                    53.05              12   \n3                52            7                    53.05              12   \n4                52            7                    53.05              12   \n...             ...          ...                      ...             ...   \n49027            33            2                    47.32              15   \n49028            33            2                    47.32              15   \n49029            33            2                    47.32              15   \n49030            33            2                    47.32              15   \n49031            33            2                    47.32              15   \n\n       type_encoded  \n0                 3  \n1                 2  \n2                 1  \n3                 2  \n4                 2  \n...             ...  \n49027             2  \n49028             3  \n49029             3  \n49030             3  \n49031             1  \n\n[49032 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test/train</th>\n      <th>sales</th>\n      <th>store_nbr</th>\n      <th>date</th>\n      <th>month</th>\n      <th>day_of_month</th>\n      <th>week_of_year</th>\n      <th>day_of_week</th>\n      <th>dcoilwtico_interpolated</th>\n      <th>family_encoded</th>\n      <th>type_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>2017-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52</td>\n      <td>7</td>\n      <td>53.05</td>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>10</td>\n      <td>2017-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52</td>\n      <td>7</td>\n      <td>53.05</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>11</td>\n      <td>2017-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52</td>\n      <td>7</td>\n      <td>53.05</td>\n      <td>12</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>12</td>\n      <td>2017-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52</td>\n      <td>7</td>\n      <td>53.05</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train</td>\n      <td>0.00</td>\n      <td>13</td>\n      <td>2017-01-01</td>\n      <td>1</td>\n      <td>1</td>\n      <td>52</td>\n      <td>7</td>\n      <td>53.05</td>\n      <td>12</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49027</th>\n      <td>train</td>\n      <td>24.00</td>\n      <td>54</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>49028</th>\n      <td>train</td>\n      <td>24.00</td>\n      <td>6</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>49029</th>\n      <td>train</td>\n      <td>42.00</td>\n      <td>7</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>49030</th>\n      <td>train</td>\n      <td>40.00</td>\n      <td>8</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>49031</th>\n      <td>train</td>\n      <td>64.00</td>\n      <td>9</td>\n      <td>2017-08-15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>33</td>\n      <td>2</td>\n      <td>47.32</td>\n      <td>15</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>49032 rows × 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense","metadata":{"execution":{"iopub.status.busy":"2023-05-20T18:48:46.171494Z","iopub.execute_input":"2023-05-20T18:48:46.171959Z","iopub.status.idle":"2023-05-20T18:48:46.180480Z","shell.execute_reply.started":"2023-05-20T18:48:46.171923Z","shell.execute_reply":"2023-05-20T18:48:46.179246Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Assuming your training data is stored in a variable named 'data'\ndataTrain_sorted = dataTrain.sort_values(by=['date','family_encoded']).reset_index(drop=True)\n\ndataTrain_sorted['family_encoded_B'] = dataTrain_sorted['family_encoded']\n\ndataTrain_sorted['change_point'] = dataTrain_sorted['family_encoded_B'].diff()\n\nchange_points = dataTrain_sorted[dataTrain_sorted['change_point'] != 0].index\n\nchange_points = change_points[1:]\n\nchange_points\n\ndataTrain_sorted['family_encoded']\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T18:48:46.724695Z","iopub.execute_input":"2023-05-20T18:48:46.725066Z","iopub.status.idle":"2023-05-20T18:48:46.808688Z","shell.execute_reply.started":"2023-05-20T18:48:46.725036Z","shell.execute_reply":"2023-05-20T18:48:46.807573Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0          0\n1          0\n2          0\n3          0\n4          0\n          ..\n404509    32\n404510    32\n404511    32\n404512    32\n404513    32\nName: family_encoded, Length: 404514, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Assuming your training data is stored in a variable named 'data'\ndataTrain_sorted = dataTrain.sort_values(by=['date','family_encoded']).reset_index(drop=True)\n\n# Use shift to compare current values with the previous values in the column\ndataTrain_sorted['change_point'] = dataTrain_sorted['family_encoded'].ne(dataTrain_sorted['family_encoded'].shift())\n\n# Find the row numbers where the change occurs\nchange_points = dataTrain_sorted[dataTrain_sorted['change_point']].index\nchange_points = change_points[1:]\n\n\ntrain_data = dataTrain_sorted.drop(['sales', 'test/train', 'date' ,'change_point'], axis=1).values\ntrain_targets = dataTrain_sorted['sales'].values # Replace this with your actual training targets\n\ninput_shape = (train_data.shape[1], 1)\nprint(train_data.shape)\nprint(input_shape)\nprint(train_targets.shape)\n\n# Normalize the input features\nX_mean = np.mean(train_data, axis=0)\nX_std = np.std(train_data, axis=0)\ntrain_data = (train_data - X_mean) / X_std\n\n\n# Split the data into separate sequences based on category or date boundaries\nsequences = []  # List to store individual sequences\ncurrent_sequence = []  # Temporary list to store the current sequence\ntargets = []\n\nreset_points = change_points  # List of indices where the reset should happen\n\nfor i, data_point in enumerate(train_data):\n    #print(i, data_point.shape)\n    current_sequence.append(data_point)\n    \n    if i in reset_points:\n        #print('in')\n        sequences.append(current_sequence)\n        current_sequence = []\n\nbatch_size = 32  # Set the desired batch size\ninput_dim = train_data.shape[1]\n        \n# Create the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(16, input_shape=(input_shape), stateful=True, batch_input_shape=(batch_size, input_dim, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nprint((batch_size, input_dim, 1))\nold_cp = 0\ni=0\n# Train the model with resetting of internal state\nfor sequence, cp in zip(sequences, change_points):\n    # Convert the sequence to numpy array format\n    sequence = np.array(sequence)\n    print('loop',sequence.shape, old_cp,cp)\n\n    \n    # Reset the internal state of the LSTM model\n    \n    # Train the model with the current sequence\n    if i == 0:\n        model.fit(sequence, train_targets[old_cp:cp+1], epochs=1,  validation_split=0.1, batch_size=32,  shuffle=False)\n    else:\n        model.fit(sequence, train_targets[old_cp:cp], epochs=1,  validation_split=0.1, batch_size=32,  shuffle=False)\n    i+=1\n    old_cp = cp\n    model.reset_states()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T19:03:04.221064Z","iopub.execute_input":"2023-05-20T19:03:04.221535Z","iopub.status.idle":"2023-05-20T19:03:20.157482Z","shell.execute_reply.started":"2023-05-20T19:03:04.221499Z","shell.execute_reply":"2023-05-20T19:03:20.156301Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"(404514, 8)\n(8, 1)\n(404514,)\n(32, 8, 1)\nloop (55, 8) 0 54\n1/2 [==============>...............] - ETA: 14s - loss: 0.7799","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Reset the internal state of the LSTM model\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Train the model with the current sequence\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mold_cp\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcp\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(sequence, train_targets[old_cp:cp], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,  shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nInvalid input_h shape: [1,32,16] [1,17,16]\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_14/lstm_14/PartitionedCall]] [Op:__inference_train_function_11830]"],"ename":"InvalidArgumentError","evalue":"Graph execution error:\n\nInvalid input_h shape: [1,32,16] [1,17,16]\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_14/lstm_14/PartitionedCall]] [Op:__inference_train_function_11830]","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_shape = train_.drop(['sales','test/train', 'date'], axis=1).values.shape\nprint(\"Data shape:\", data_shape)\ntrain = train_.drop(['sales','test/train', 'date'], axis=1).shape\n\nbackcast_length = 365","metadata":{"execution":{"iopub.status.busy":"2023-05-20T09:59:54.348788Z","iopub.execute_input":"2023-05-20T09:59:54.351165Z","iopub.status.idle":"2023-05-20T09:59:54.376992Z","shell.execute_reply.started":"2023-05-20T09:59:54.351090Z","shell.execute_reply":"2023-05-20T09:59:54.375966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model architecture\nmodel = NBeatsNet(input_dim=1, output_dim=1, stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n                  nb_blocks_per_stack=3, forecast_length=5, backcast_length=10,\n                  thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=64)\n\n# Compile the model\ndef rmsle_loss(y_true, y_pred):\n    \n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = K.square(y_true_log - y_pred_log)\n    mean_squared_diff = K.mean(squared_diff, axis=-1)\n    rmsle = K.sqrt(mean_squared_diff)\n    return rmsle\n\n# Compile the model with RMSLE loss\nmodel.compile(loss=rmsle_loss, optimizer='adam')#model.compile(loss='mean_squared_error', optimizer='adam', metrics=[rmsle])\n\n# Set training parameters\nbatch_size = 64\nepochs = 50\nearly_stopping_patience = 10","metadata":{"execution":{"iopub.status.busy":"2023-05-19T16:03:33.724711Z","iopub.execute_input":"2023-05-19T16:03:33.725116Z","iopub.status.idle":"2023-05-19T16:03:34.329669Z","shell.execute_reply.started":"2023-05-19T16:03:33.725088Z","shell.execute_reply":"2023-05-19T16:03:34.327173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Prepare the input and target data\n#X_train = dataTrain['sales'].values.reshape(-1, 1)\ny_train = train_['sales'].values.reshape(-1, 1)\n#X_train = dataTrain.drop('sales', axis=1).values.reshape(-1, 1)\nnum_features = train_.drop(['sales','test/train', 'date'], axis=1).shape[1]\nnum_features\n\nprint (365 ,num_features)\nX_train = tf.convert_to_tensor(train_.drop(['sales','test/train', 'date'], axis=1).values.reshape((-1, 365, num_features)), dtype=tf.float32)\n\n\n\n# Create an early stopping callback\nearly_stopping = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n\n# Fit the model\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:56:56.646253Z","iopub.execute_input":"2023-05-19T15:56:56.647983Z","iopub.status.idle":"2023-05-19T15:56:56.683534Z","shell.execute_reply.started":"2023-05-19T15:56:56.647895Z","shell.execute_reply":"2023-05-19T15:56:56.682075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Define the parameters\ninput_dim = train_.drop(['sales', 'test/train', 'date'], axis=1).shape[1]\nbackcast_length = 5\nforecast_length = 5\n\n# Reduce the data size to fit the desired shape\nnum_samples = train_.shape[0] // (backcast_length + forecast_length)\nnum_samples = min(num_samples, 3067)  # Set the desired number of samples here\n\n# Reshape the data\ntrain_data = train_.drop(['sales', 'test/train', 'date'], axis=1).values[:num_samples * (backcast_length + forecast_length)]\ntrain_data = train_data.reshape((num_samples, backcast_length + forecast_length, input_dim))\n\n# Convert the reshaped data to TensorFlow tensor\nX_train = tf.convert_to_tensor(train_data, dtype=tf.float32)\n\n# Create the N-BEATS model\nmodel = NBeatsNet(input_dim=input_dim, backcast_length=backcast_length, forecast_length=forecast_length,\n                  stack_types=(NBeatsNet.GENERIC_BLOCK,), nb_blocks_per_stack=3, thetas_dim=(4,), share_weights_in_stack=False)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Fit the model to the data\nmodel.fit(X_train, X_train, epochs=10, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-05-19T16:10:17.275630Z","iopub.execute_input":"2023-05-19T16:10:17.276080Z","iopub.status.idle":"2023-05-19T16:10:19.117990Z","shell.execute_reply.started":"2023-05-19T16:10:17.276046Z","shell.execute_reply":"2023-05-19T16:10:19.115090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom nbeats_keras.model import NBeatsNet\n\nbackcast_length \n\n# Define the NBeatsNet model\nmodel = NBeatsNet(input_dim=1, output_dim=1, stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n                  nb_blocks_per_stack=3, forecast_length=7, backcast_length=backcast_length,\n                  thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=64)\n\n# Define the RMSLE loss function\ndef rmsle_loss(y_true, y_pred):\n    epsilon = 1e-7  # Small constant to prevent division by zero\n    y_true = tf.maximum(y_true, epsilon)  # Ensure y_true is always positive\n    y_pred = tf.maximum(y_pred, epsilon)  # Ensure y_pred is always positive\n    y_true_log = tf.math.log1p(y_true)\n    y_pred_log = tf.math.log1p(y_pred)\n    squared_diff = tf.square(y_true_log - y_pred_log)\n    mean_squared_diff = tf.reduce_mean(squared_diff, axis=-1)\n    rmsle = tf.sqrt(mean_squared_diff)\n    return rmsle\n\n\n\n# Define the parameters\ninput_dim = train_.shape[1]\nbackcast_length = 10\nforecast_length = 5\n\n# Reshape the data\nnum_samples = train_.shape[0]\ntotal_length = backcast_length + forecast_length\nprint(train_.shape)\nprint(num_samples, total_length, input_dim)\n\ntrain_data = np.reshape(train_, (num_samples, total_length, input_dim))\nprint(train_.shape())\n","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:53:57.225198Z","iopub.execute_input":"2023-05-19T15:53:57.225608Z","iopub.status.idle":"2023-05-19T15:53:57.770741Z","shell.execute_reply.started":"2023-05-19T15:53:57.225582Z","shell.execute_reply":"2023-05-19T15:53:57.768900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_.drop(['sales','test/train', 'date'], axis=1).shape","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:43:54.905179Z","iopub.execute_input":"2023-05-19T15:43:54.905567Z","iopub.status.idle":"2023-05-19T15:43:54.914055Z","shell.execute_reply.started":"2023-05-19T15:43:54.905541Z","shell.execute_reply":"2023-05-19T15:43:54.913067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the input data for prediction\nX_test = test_['sales'].values.reshape(-1, 1)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Add the predictions to the test data\ntest_data['predicted_target'] = predictions.flatten()","metadata":{"execution":{"iopub.status.busy":"2023-05-19T15:54:27.737185Z","iopub.execute_input":"2023-05-19T15:54:27.737622Z","iopub.status.idle":"2023-05-19T15:54:27.874954Z","shell.execute_reply.started":"2023-05-19T15:54:27.737587Z","shell.execute_reply":"2023-05-19T15:54:27.872866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}