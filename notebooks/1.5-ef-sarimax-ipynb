{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-23T18:08:38.913816Z","iopub.status.busy":"2023-05-23T18:08:38.913308Z","iopub.status.idle":"2023-05-23T18:09:00.828654Z","shell.execute_reply":"2023-05-23T18:09:00.826972Z","shell.execute_reply.started":"2023-05-23T18:08:38.913778Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["config\n"]}],"source":["#!pip install numpy==1.22.3 --force-reinstall\n","\n","#!pip install keras\n","#!pip install tensorflow\n","# BASE\n","# ------------------------------------------------------\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import gc\n","import warnings\n","from pathlib import Path\n","#!pip install xgboost\n","from tqdm import tqdm\n","\n","import sklearn\n","from sklearn import linear_model\n","from sklearn.linear_model  import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","import xgboost as xgb\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split, TimeSeriesSplit \n","from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score\n","pd.set_option('display.max_colwidth', None)\n","import statsmodels.api as sm\n","import importlib.util\n","\n","import numpy as np\n","from sklearn.model_selection import TimeSeriesSplit\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","\n","\n","\n","import numpy as np\n","#from keras.optimizers import Adam\n","#from keras.callbacks import EarlyStopping\n","#from nbeats_keras.model import NBeatsNet\n","#import tensorflow as tf\n","#from keras import backend as K\n","\n","\n","\n","from pandas import datetime\n","from pandas import read_csv\n","from pandas import DataFrame\n","from statsmodels.tsa.arima.model import ARIMA\n","from matplotlib import pyplot\n","\n","# DATA VISUALIZATION\n","# ------------------------------------------------------\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# CONFIGURATIONS\n","# ------------------------------------------------------\n","pd.set_option('display.max_columns', None)\n","pd.options.display.float_format = '{:.2f}'.format\n","warnings.filterwarnings('ignore')\n","print('config')\n","#from pandas.core.common import SettingWithCopyWarning\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T18:09:00.834055Z","iopub.status.busy":"2023-05-23T18:09:00.833421Z","iopub.status.idle":"2023-05-23T18:10:03.780300Z","shell.execute_reply":"2023-05-23T18:10:03.778863Z","shell.execute_reply.started":"2023-05-23T18:09:00.834005Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Parent directory: c:\\Users\\eflanag\\OneDrive - FTI Consulting\\Documents\\Projects\\Ibex_EF\\IbexV2\\ibexgit\\kaggle-store-sales\n","(3029400, 81)\n","complete\n"]}],"source":["import os\n","import importlib.util\n","\n","def kaggle_inport():\n","    for dirname, _, filenames in os.walk('/kaggle/input'):\n","        for filename in filenames:\n","            if filename == 'data_setup_kaggle.py':\n","                print('Found data_setup_kaggle.py')\n","                module_name = 'data_setup_kaggle'\n","                module_path = os.path.join(dirname, filename)\n","                \n","                spec = importlib.util.spec_from_file_location(module_name, module_path)\n","                data_setup = importlib.util.module_from_spec(spec)\n","                spec.loader.exec_module(data_setup)\n","                \n","                # Execute a function from the imported module and assign its result to 'd'\n","                d = data_setup.get_data()\n","                return d\n","                # Now you can use the imported module\n","                # ...\n","\n","\n","import importlib.util\n","\n","# Specify the absolute path to the data_setup.py file\n","data_setup_path = '../src/data_setup.py'\n","\n","# Load the module from the file\n","spec = importlib.util.spec_from_file_location('data_setup', data_setup_path)\n","data_setup = importlib.util.module_from_spec(spec)\n","spec.loader.exec_module(data_setup)    \n","d = data_setup.get_data()\n","\n","            \n","# Compile the model\n","def rmsle_loss(y_true, y_pred):\n","\n","    y_true_log = tf.math.log1p(y_true)\n","    y_pred_log = tf.math.log1p(y_pred)\n","    squared_diff = K.square(y_true_log - y_pred_log)\n","    mean_squared_diff = K.mean(squared_diff, axis=-1)\n","    rmsle = K.sqrt(mean_squared_diff)\n","    return rmsle"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T18:10:03.782438Z","iopub.status.busy":"2023-05-23T18:10:03.781971Z","iopub.status.idle":"2023-05-23T18:10:03.800257Z","shell.execute_reply":"2023-05-23T18:10:03.798447Z","shell.execute_reply.started":"2023-05-23T18:10:03.782399Z"},"trusted":true},"outputs":[],"source":["def reduce_mem_usage(df):\n","    \"\"\" iterate through all the columns of a dataframe and modify the data type\n","        to reduce memory usage.        \n","    \"\"\"\n","    #start_mem = df.memory_usage().sum() / 1024**2\n","    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n","\n","    for col in df.columns:\n","        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","\n","        col_type = df[col].dtype\n","        if col_type != object:\n","            if pd.api.types.is_categorical_dtype(df[col]):\n","                # Convert categorical column to ordered categorical\n","                i=0\n","            elif col_type in numerics:\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","                if str(col_type)[:3] == 'int':\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        df[col] = df[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        df[col] = df[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        df[col] = df[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        df[col] = df[col].astype(np.int64)  \n","                else:\n","                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                        df[col] = df[col].astype(np.float16)\n","                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                        df[col] = df[col].astype(np.float32)\n","                    else:\n","                        df[col] = df[col].astype(np.float64)\n","\n","    #end_mem = df.memory_usage().sum() / 1024**2\n","    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n","    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n","    gc.collect()\n","    return df"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T18:10:03.805236Z","iopub.status.busy":"2023-05-23T18:10:03.804261Z","iopub.status.idle":"2023-05-23T18:10:08.560723Z","shell.execute_reply":"2023-05-23T18:10:08.559226Z","shell.execute_reply.started":"2023-05-23T18:10:03.805165Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["metrics_df = pd.DataFrame(columns=['Model', 'family_encoded', 'cols', 'MAE', 'MSE', 'RMSE','RMSLE', 'R2'])\n","\n","# preform test train split\n","cols = ['test/train' ,'sales', 'store_nbr', 'type', 'family', 'date', 'month','day_of_month', 'week_of_year','day_of_week', 'dcoilwtico_interpolated']\n","#cols = ['test/train' ,'sales', 'family', 'date', 'month','day_of_month', 'week_of_year','day_of_week', 'type', 'dcoilwtico_interpolated']\n","\n","dataTrainTest = d[cols]\n","\n","dataTrain = dataTrainTest[dataTrainTest['test/train'] ==  'train'].reset_index(drop=True)\n","dataTest = dataTrainTest[dataTrainTest['test/train'] ==  'test'].reset_index(drop=True)\n","\n","dataTrain\n","\n","from sklearn.preprocessing import LabelEncoder\n","encoder = LabelEncoder()\n","dataTrain[\"family_encoded\"] = encoder.fit_transform(dataTrain[\"family\"])\n","dataTrain.drop('family', axis=1, inplace=True)\n","\n","dataTrain[\"type\"]\n","from sklearn.preprocessing import LabelEncoder\n","encoder = LabelEncoder()\n","dataTrain[\"type_encoded\"] = encoder.fit_transform(dataTrain[\"type\"])\n","dataTrain.drop('type', axis=1, inplace=True)\n","\n","train_ = (dataTrain)\n","test_ = (dataTest)\n","\n","#train_, test_ = train_test_split(dataTrain.dropna().reset_index(drop=True), test_size=0.2, random_state=42)\n","\n","train_ = reduce_mem_usage(train_)\n","test_ = reduce_mem_usage(test_)\n","\n","del d, dataTest, dataTrainTest\n","gc.collect()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T18:10:08.562717Z","iopub.status.busy":"2023-05-23T18:10:08.562322Z","iopub.status.idle":"2023-05-23T18:10:08.570696Z","shell.execute_reply":"2023-05-23T18:10:08.569310Z","shell.execute_reply.started":"2023-05-23T18:10:08.562683Z"},"trusted":true},"outputs":[],"source":["cols_out = list(train_.columns)\n","cols_out.append('y_pred')\n","cols_out.append('y_pred_raw')\n","cols_out.append('order')\n","cols_out.append('seasonal_order')\n","cols_out.append('fit_time')"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T18:10:08.572620Z","iopub.status.busy":"2023-05-23T18:10:08.572173Z","iopub.status.idle":"2023-05-23T18:10:08.589110Z","shell.execute_reply":"2023-05-23T18:10:08.587780Z","shell.execute_reply.started":"2023-05-23T18:10:08.572586Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['test/train',\n"," 'sales',\n"," 'store_nbr',\n"," 'date',\n"," 'month',\n"," 'day_of_month',\n"," 'week_of_year',\n"," 'day_of_week',\n"," 'dcoilwtico_interpolated',\n"," 'family_encoded',\n"," 'type_encoded',\n"," 'y_pred',\n"," 'y_pred_raw',\n"," 'order',\n"," 'seasonal_order',\n"," 'fit_time']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["cols_out"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# from joblib import Parallel, delayed\n","# import sys\n","\n","# # Define a helper function to calculate metrics\n","# def calculate_metrics(test_ARIMA, y_pred_ARIMA):\n","#     # Calculate metrics\n","#     mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n","#     mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n","#     rmse = np.sqrt(mse)\n","#     r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n","#     y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n","#     rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n","#     return mae, mse, rmse, rmsle, r2\n","\n","# def process_family( o , so, family):\n","#     global dataTrain_out, dataTrain\n","    \n","#     dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family].copy()\n","#     last_month = dataTrainFamily['date'].max().to_period('M')\n","#     start_date = last_month.start_time + pd.DateOffset(days=1)\n","#     train_ = dataTrainFamily[dataTrainFamily['date'] < start_date].copy()\n","#     test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date].copy()\n","\n","#     series = train_.copy(deep=True)\n","#     series.set_index('date', inplace=True)\n","#     series = series[['sales']]\n","#     series.index = series.index.to_period('M')\n","\n","#     test_inner = test_.copy(deep=True)\n","#     test_inner.set_index('date', inplace=True)\n","#     test_inner.index = test_inner.index.to_period('M')\n","\n","#     test_ARIMA = test_inner['sales']\n","\n","#     series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n","#     test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n","\n","#     model = SARIMAX(series['sales'], order=0, seasonal_order=so,\n","#                     initialization='approximate_diffuse', enforce_stationarity=False)\n","#     model_fit = model.fit(disp=False)\n","\n","#     y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']))\n","#     y_pred_ARIMA = np.round(y_pred_ARIMA).astype(int)\n","#     mae, mse, rmse, rmsle, r2 = calculate_metrics(test_ARIMA, y_pred_ARIMA)\n","\n","#     result = pd.DataFrame([[family, mae, mse, rmse, rmsle, r2]],\n","#                           columns=['family_encoded', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n","    \n","#     test_['y_pred'] = y_pred_ARIMA.values\n","#     #test_.iloc[:,'seasonal_order'] = so\n","#     test_['seasonal_order'] = test_.apply(lambda row: np.array(so), axis=1)\n","#     test_['order'] = test_.apply(lambda row: np.array(o), axis=1)\n","#     test_['fit_time'] = int(fitting_time)\n","    \n","#     dataTrain_out = pd.concat([dataTrain_out, test_], ignore_index=True)\n","\n","\n","#     del model_fit, y_pred_ARIMA, test_, model, series, test_inner, dataTrainFamily, train_, test_ARIMA\n","#     gc.collect()\n","\n","#     return dataTrain_out\n","\n","\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-23T18:10:08.591571Z","iopub.status.busy":"2023-05-23T18:10:08.591028Z","iopub.status.idle":"2023-05-23T18:10:37.657266Z","shell.execute_reply":"2023-05-23T18:10:37.655298Z","shell.execute_reply.started":"2023-05-23T18:10:08.591534Z"},"trusted":true},"outputs":[],"source":["from joblib import Parallel, delayed\n","import sys\n","\n","\n","\n","# Define a helper function to calculate metrics\n","def calculate_metrics(test_ARIMA, y_pred_ARIMA):\n","    mae = mean_absolute_error(test_ARIMA, y_pred_ARIMA)\n","    mse = mean_squared_error(test_ARIMA, y_pred_ARIMA)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(test_ARIMA, y_pred_ARIMA)\n","    y_pred_ARIMA[y_pred_ARIMA < 0] = 0\n","    rmsle = np.sqrt(mean_squared_log_error(test_ARIMA, y_pred_ARIMA))\n","    return mae, mse, rmse, rmsle, r2\n","\n","dataTrain = dataTrain.sort_values(['family_encoded', 'date', 'type_encoded'])\n","dataTrain_out = pd.DataFrame(columns=cols_out)\n","metrics_df = pd.DataFrame(columns=['Model', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n","\n","#def process_family(family):\n","#    global dataTrain\n","# Define the process_family function\n","def process_family( o , so, family):\n","    global dataTrain_out, metrics_df, dataTrain\n","    #warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)\n","    warnings.filterwarnings(\"ignore\")\n","    \n","    print(family)\n","    sys.stdout.flush()\n","    dataTrainFamily = dataTrain[dataTrain['family_encoded'] == family].copy()\n","    last_month = dataTrainFamily['date'].max().to_period('M')\n","    start_date = last_month.start_time + pd.DateOffset(days=1)\n","    train_ = dataTrainFamily[dataTrainFamily['date'] < start_date].copy()\n","    test_ = dataTrainFamily[dataTrainFamily['date'] >= start_date].copy()\n","\n","    series = train_.copy(deep=True)\n","    cols = train_.columns.tolist()\n","    series.set_index('date', inplace=True)\n","    series.index = series.index.to_period('M')\n","    exogenous = series.drop(['test/train', 'sales'], axis=1).copy()\n","    exogenous['dcoilwtico_interpolated'] = exogenous['dcoilwtico_interpolated'].astype('int8')\n","    exogenous['type_encoded'] = pd.Categorical(exogenous['type_encoded'])\n","    exogenous['family_encoded'] = pd.Categorical(exogenous['family_encoded'])\n","\n","    series = series[['sales']]\n","\n","\n","\n","    test_inner = test_.copy(deep=True)\n","    test_inner.set_index('date', inplace=True)\n","    test_inner.index = test_inner.index.to_period('M')\n","    test_exog = test_inner.drop(['test/train', 'sales'], axis=1).copy()\n","    test_exog['dcoilwtico_interpolated'] = test_exog['dcoilwtico_interpolated'].astype('int8')\n","    test_exog['type_encoded'] = pd.Categorical(test_exog['type_encoded'])\n","\n","    del train_, dataTrainFamily\n","    gc.collect()\n","\n","\n","\n","    \n","    \n","    test_ARIMA = test_inner['sales']\n","\n","    series['sales'] = pd.to_numeric(series['sales'], downcast='integer')\n","    test_ARIMA = pd.to_numeric(test_ARIMA, downcast='integer')\n","\n","\n","    # try:\n","    start_time = time.time()\n","\n","    model = SARIMAX(series['sales'], order=(o), seasonal_order=(so), exog = exogenous, initialization='approximate_diffuse',enforce_stationarity=False)\n","    model_fit = model.fit(disp=False)#, method='nm', max_iter=1000)\n","\n","    # Calculate the fitting time\n","    fitting_time = time.time() - start_time\n","    # Print the fitting time\n","    print('Family', family, 'Fitting time:', fitting_time, \"seconds\")\n","    sys.stdout.flush()\n","\n","    y_pred_ARIMA = model_fit.forecast(steps=len(test_inner['sales']), exog = test_exog)\n","\n","    y_pred_ARIMA = np.round(y_pred_ARIMA).astype(int)\n","    mae, mse, rmse, rmsle, r2 = calculate_metrics(test_ARIMA, y_pred_ARIMA)\n","    sys.stdout.flush()\n","    test_['y_pred_raw'] = y_pred_ARIMA.values\n","\n","    result = pd.DataFrame([[str(so), family, cols, mae, mse, rmse, rmsle, r2]], columns=['Model', 'family_encoded', 'cols', 'MAE', 'MSE', 'RMSE', 'RMSLE', 'R2'])\n","    #metrics_df = metrics_df.append(result, ignore_index=True)\n","\n","    test_['y_pred'] = y_pred_ARIMA.values\n","    #test_.iloc[:,'seasonal_order'] = so\n","    test_['seasonal_order'] = test_.apply(lambda row: np.array(so), axis=1)\n","    test_['order'] = test_.apply(lambda row: np.array(o), axis=1)\n","    test_['fit_time'] = int(fitting_time)\n","\n","\n","    #test_.iloc[:,'order'] = o\n","\n","    #test_['order'] = [o]\n","\n","    dataTrain_out = pd.concat([dataTrain_out, test_], ignore_index=True)\n","    print('Family', family, ' mean -', result['RMSLE'].mean())\n","\n","    #print('rmsle',rmsle)\n","    #print('test_[y_pred]',test_['y_pred'])\n","    #print('dataTrain_out[y_pred]',dataTrain_out['y_pred'])\n","\n","    sys.stdout.flush()\n","\n","    del model_fit, y_pred_ARIMA, test_ , model, series, test_inner, result, mae, mse, rmse, rmsle, r2 \n","    gc.collect()\n","    #except Exception as e:\n","     #   print(e)\n","\n","    return (dataTrain_out)\n","    \n","#Parallel(n_jobs=4, prefer='processes')(delayed(process_family)(family) for family in dataTrain['family_encoded'].unique())\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 0, 0) [1, 0, 0, 12]\n","(0, 1, 0) [1, 0, 0, 12]\n","(1, 0, 1) [1, 0, 0, 12]\n"]}],"source":["\n","order_values = [(1, 0, 0), (0, 1, 0), (1, 0, 1)]\n","seasonal_order_values = [(1, 0, 0, 12), (0, 1, 1, 12), (1, 0, 1, 12)]\n","\n","for o in (order_values):\n","    so = [1, 0, 0, 12]\n","    print (o, so)\n","\n","    dataTrain_result = Parallel(n_jobs=8, prefer='processes')(delayed(process_family)(o, so, family) for family in dataTrain['family_encoded'].unique())\n","\n","    #dataTrain_result = Parallel(n_jobs=8, prefer='processes')(delayed(process_family)(o, so, family) for family in [12, 13])\n","\n","    dataTrain_out = pd.concat([dataTrain_out] + dataTrain_result, ignore_index=True)\n","    dataTrain_out = dataTrain_out.sort_values(['date', 'family_encoded'])\n","\n","    name = str(so) + str(o)\n","\n","    dataTrain_out.to_csv('SARIMAX'+name+'.csv')\n","    dataTrain_out = pd.DataFrame(columns=cols_out)\n","\n","\n","\n","    del dataTrain_result\n","    gc.collect()\n","    \n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 0, 0) [0, 1, 1, 12]\n","(0, 1, 0) [0, 1, 1, 12]\n","(1, 0, 1) [0, 1, 1, 12]\n"]},{"data":{"text/plain":["0"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["dataTrain_out = pd.DataFrame(columns=cols_out)\n","\n","for o in (order_values):\n","    so = [0, 1, 1, 12]\n","    print (o, so)\n","\n","    \n","\n","    dataTrain_result = Parallel(n_jobs=8, prefer='processes')(delayed(process_family)(o, so, family) for family in dataTrain['family_encoded'].unique())\n","\n","    #dataTrain_result = Parallel(n_jobs=8, prefer='processes')(delayed(process_family)(o, so, family) for family in [12, 13])\n","\n","    dataTrain_out = pd.concat([dataTrain_out] + dataTrain_result, ignore_index=True)\n","    dataTrain_out = dataTrain_out.sort_values(['date', 'family_encoded'])\n","\n","    name = str(so) + str(o)\n","\n","    dataTrain_out.to_csv('SARIMAX'+name+'.csv')\n","    dataTrain_out = pd.DataFrame(columns=cols_out)\n","\n","\n","\n","    del dataTrain_result\n","    gc.collect()\n","\n","\n","gc.collect()\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 0, 0) [1, 0, 1, 12]\n","(0, 1, 0) [1, 0, 1, 12]\n","(1, 0, 1) [1, 0, 1, 12]\n"]},{"data":{"text/plain":["0"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["dataTrain_out = pd.DataFrame(columns=cols_out)\n","\n","for o in (order_values):\n","    so = [1, 0, 1, 12]\n","    print (o, so)\n","\n","\n","    dataTrain_result = Parallel(n_jobs=8, prefer='processes')(delayed(process_family)(o, so, family) for family in dataTrain['family_encoded'].unique())\n","\n","    #dataTrain_result = Parallel(n_jobs=8, prefer='processes')(delayed(process_family)(o, so, family) for family in [12, 13])\n","\n","    dataTrain_out = pd.concat([dataTrain_out] + dataTrain_result, ignore_index=True)\n","    dataTrain_out = dataTrain_out.sort_values(['date', 'family_encoded'])\n","\n","    name = str(so) + str(o)\n","\n","    dataTrain_out.to_csv('SARIMAX'+name+'.csv')\n","    dataTrain_out = pd.DataFrame(columns=cols_out)\n","\n","\n","\n","    del dataTrain_result\n","    gc.collect()\n","    \n","\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = pd.read_csv('/kaggle/working/SARIMAX.csv')\n","df2 = pd.read_csv('/kaggle/working/SARIMAX2.csv')\n","df3 = pd.read_csv('/kaggle/working/SARIMAX3.csv')\n","\n","dataTrain_out = pd.concat([df1, df2, df3], ignore_index=True)\n","\n","dataTrain_out.to_csv('combined_df.csv')\n","del df1, df2, df3\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.188848Z","iopub.status.idle":"2023-05-23T18:08:31.189569Z","shell.execute_reply":"2023-05-23T18:08:31.189258Z","shell.execute_reply.started":"2023-05-23T18:08:31.189224Z"},"trusted":true},"outputs":[],"source":["dataTrain_out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.192191Z","iopub.status.idle":"2023-05-23T18:08:31.193274Z","shell.execute_reply":"2023-05-23T18:08:31.192956Z","shell.execute_reply.started":"2023-05-23T18:08:31.192925Z"},"trusted":true},"outputs":[],"source":["#dataTrain_out['order_cat'] = pd.cat\n","dataTrain_out['order'] = pd.Categorical(dataTrain_out['order'].apply(tuple))\n","dataTrain_out['seasonal_order'] = pd.Categorical(dataTrain_out['seasonal_order'].apply(lambda x: tuple(x)))\n","\n","dataTrain_out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.194945Z","iopub.status.idle":"2023-05-23T18:08:31.195636Z","shell.execute_reply":"2023-05-23T18:08:31.195343Z","shell.execute_reply.started":"2023-05-23T18:08:31.195312Z"},"trusted":true},"outputs":[],"source":["dataTrain_out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.197421Z","iopub.status.idle":"2023-05-23T18:08:31.198990Z","shell.execute_reply":"2023-05-23T18:08:31.198775Z","shell.execute_reply.started":"2023-05-23T18:08:31.198750Z"},"trusted":true},"outputs":[],"source":["group = dataTrain_out.groupby(['family_encoded', \n","                               ('order'), ('seasonal_order')])\n","\n","for g in group:\n","    df = g[1]  # Access the DataFrame from the tuple using index 1\n","    #print(df)\n","    mae, mse, rmse, rmsle, r2 = calculate_metrics((df['sales']).to_numpy(), (df['y_pred']).to_numpy())\n","    dataTrain_out.loc[dataTrain_out['family_encoded'] ==df['family_encoded'].iloc[0], ['mae', 'mse', 'rmse', 'rmsle', 'r2']] = mae, mse, rmse, rmsle, r2\n","\n"," \n","    print('Family: {}, Order: {}, Seasonal Order: {}, MAE: {:.2f}, MSE: {:.2f}, RMSE: {:.2f}, RMSLE: {:.2f}, R2: {:.2f}'.format(\n","        df['family_encoded'].iloc[0], df['order'].iloc[0], df['seasonal_order'].iloc[0], mae, mse, rmse, rmsle, r2))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.200471Z","iopub.status.idle":"2023-05-23T18:08:31.201454Z","shell.execute_reply":"2023-05-23T18:08:31.201180Z","shell.execute_reply.started":"2023-05-23T18:08:31.201152Z"},"trusted":true},"outputs":[],"source":["dataTrain_out['y_pred'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.202713Z","iopub.status.idle":"2023-05-23T18:08:31.203846Z","shell.execute_reply":"2023-05-23T18:08:31.203621Z","shell.execute_reply.started":"2023-05-23T18:08:31.203595Z"},"trusted":true},"outputs":[],"source":["dataTrain_out.to_csv('SARIMAX.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.205705Z","iopub.status.idle":"2023-05-23T18:08:31.206154Z","shell.execute_reply":"2023-05-23T18:08:31.205977Z","shell.execute_reply.started":"2023-05-23T18:08:31.205957Z"},"trusted":true},"outputs":[],"source":["# small_number = 1e-10  # Define the small number you want to replace 0 with\n","\n","# dataTrain_out['y_pred_near_0'] = np.where(dataTrain_out['y_pred'] == 0, small_number, dataTrain_out['y_pred'])\n","\n","# dataTrain_out['%_diff'] = dataTrain_out['sales'] / dataTrain_out['y_pred_near_0']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.208190Z","iopub.status.idle":"2023-05-23T18:08:31.208720Z","shell.execute_reply":"2023-05-23T18:08:31.208532Z","shell.execute_reply.started":"2023-05-23T18:08:31.208505Z"},"trusted":true},"outputs":[],"source":["# mean_diff_by_family = dataTrain_out.groupby('family_encoded')['%_diff'].mean()\n","# mean_diff_by_family"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.210059Z","iopub.status.idle":"2023-05-23T18:08:31.210533Z","shell.execute_reply":"2023-05-23T18:08:31.210342Z","shell.execute_reply.started":"2023-05-23T18:08:31.210322Z"},"trusted":true},"outputs":[],"source":["# dataTrain_out['%_diff_n0_0'] = np.where(\n","#     (dataTrain_out['sales'] != 0) | (dataTrain_out['y_pred'] != 0),\n","#     dataTrain_out['sales'] / dataTrain_out['y_pred_near_0'],\n","#     np.nan\n","# )\n","\n","# dataTrain_out['%_diff_n0_0'].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:08:31.213307Z","iopub.status.idle":"2023-05-23T18:08:31.214297Z","shell.execute_reply":"2023-05-23T18:08:31.213992Z","shell.execute_reply.started":"2023-05-23T18:08:31.213960Z"},"trusted":true},"outputs":[],"source":["# mean_diff_by_family['family_encoded'] = mean_diff_by_family.index\n","# mean_diff_by_family = mean_diff_by_family.reset_index(drop=True)\n","# #metrics_df\n","# #result\n","# mean_diff_by_family\n","# dataTrain_out_merged = dataTrain_out.merge(mean_diff_by_family, on='family_encoded')\n","# dataTrain_out_filtered = dataTrain_out_merged[dataTrain_out_merged['mean'] <= 2]\n","\n","# mean_diff_by_family"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
