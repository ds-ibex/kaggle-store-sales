{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Modelling\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "import lightgbm as ltb\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV)\n",
    "\n",
    "\n",
    "# Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Src modules\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Add the parent directory to the Python path so we can import src modules\n",
    "from src.data_setup import *\n",
    "from src.model_evaluation import model_eval_pipeline,calc_root_mean_squared_error,calc_root_mean_squared_log_error\n",
    "\n",
    "#Path Variables\n",
    "ROOT_PATH =Path(os.path.dirname(os.getcwd()))\n",
    "DATA_PATH = ROOT_PATH / 'data'\n",
    "SUBMISSION_PATH = DATA_PATH / 'submissions'\n",
    "MODEL_PATH=ROOT_PATH/'models/'\n",
    "DECISIONTREE_PATH=MODEL_PATH/'DecisionTree/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality EDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load train,val and test data\n",
    "train, test, stores, transactions = get_data()\n",
    "train = clean_train(train)\n",
    "\n",
    "#Transform train data to Decision Tree Input\n",
    "df_train =Transform_Data_For_DT(train,60,True)\n",
    "#Create basic, trend and seasonality features \n",
    "df_feats=DT_features(df_train,True)\n",
    "\n",
    "#Create the split train/validation\n",
    "df_training,df_validation=train_val_split(df_train)\n",
    "df_feats_use,df_feats_validation=train_val_split(df_feats)\n",
    "\n",
    "#Remove Date column for features dataframe\n",
    "df_feats_validation=df_feats_validation.drop(columns={'date'})\n",
    "df_feats_use=df_feats_use.drop(columns={'date'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Load Model if they exists\n",
    "#lgbm = load_model(DECISIONTREE_PATH,'lgbm.pkl')\n",
    "#rf = load_model(DECISIONTREE_PATH,'rf.pkl')\n",
    "#gbr = load_model(DECISIONTREE_PATH,'gbr.pkl')\n",
    "\n",
    "#Run each model on the validation data\n",
    "lgbm_pred=lgbm.predict(df_feats_validation)\n",
    "rf_pred=rf.predict(df_feats_validation)\n",
    "gbr_pred=gbr.predict(df_feats_validation)\n",
    "\n",
    "#Zero all negative values\n",
    "for i in range(len(lgbm_pred)):\n",
    "    lgbm_pred[i]=max(0,lgbm_pred[i])\n",
    "for i in range(len(rf_pred)):\n",
    "    rf_pred[i]=max(0,rf_pred[i])\n",
    "for i in range(len(gbr_pred)):\n",
    "    gbr_pred[i]=max(0,gbr_pred[i])\n",
    "\n",
    "#Measure RMSLE Score\n",
    "lgbm_score = calc_root_mean_squared_log_error(df_validation['target'], lgbm_pred)\n",
    "rf_score = calc_root_mean_squared_log_error(df_validation['target'], rf_pred)\n",
    "gbr_score = calc_root_mean_squared_log_error(df_validation['target'], gbr_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Comparison"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
